{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float64) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float64) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_14196\\3146107952.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(x*x), 2*x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.006760443547122)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float64)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int64)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float64)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int64)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float64)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float64)\n",
    "target_index = np.ones(batch_size, dtype=np.int32)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397363\n",
      "Epoch 1, loss: 2.330354\n",
      "Epoch 2, loss: 2.311002\n",
      "Epoch 3, loss: 2.303897\n",
      "Epoch 4, loss: 2.303257\n",
      "Epoch 5, loss: 2.302898\n",
      "Epoch 6, loss: 2.302564\n",
      "Epoch 7, loss: 2.301815\n",
      "Epoch 8, loss: 2.301252\n",
      "Epoch 9, loss: 2.301256\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c282140b90>]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQfdJREFUeJzt3Ql8VOXZ9/Frsu8JIWQjAcIOhh1EBAEVWeqj0OpbtSqiVl4RfEpdi1at6CvVrtpa7GMtSC1C9QFpUVFkFVmUTXYkQEiAhEDITvbM+7nuMDGBBJKQ5Mxkft/PZ5gJc2Zyz8lMzj/XvRyb3W63CwAAgBPzsLoBAAAAl0NgAQAATo/AAgAAnB6BBQAAOD0CCwAAcHoEFgAA4PQILAAAwOkRWAAAgNPzklagoqJCTp48KcHBwWKz2axuDgAAqAdduzYvL09iY2PFw8Oj9QcWDSvx8fFWNwMAADRCamqqxMXFtf7AopUVxwsOCQmxujkAAKAecnNzTcHBcRxv9YHF0Q2kYYXAAgCAa6nPcA4G3QIAAKdHYAEAAE6PwAIAAJwegQUAADg9AgsAAHB6BBYAAOD0CCwAAMDpEVgAAIDTI7AAAACnR2ABAABOj8ACAACcHoEFAAA4PQLLJeQXl8lvPzsoT3+4S+x2u9XNAQDAbRFYLsHLwyZ/XpMki7emSk5hqdXNAQDAbRFYLsHP21PaBvqY2yezi6xuDgAAbovAchmxYf7m+mR2odVNAQDAbRFYLiM2zM9cn8whsAAAYBUCSz0rLCeosAAAYBkCy2XEhjq6hBjDAgCAVQgs9aywpFFhAQDANQLLnDlzZMiQIRIcHCyRkZEyadIkOXjw4GUfl52dLdOnT5eYmBjx9fWV7t27yyeffFJjmzfffFM6deokfn5+MnToUPn666/FqcawEFgAAHCNwLJu3ToTPDZv3iwrV66U0tJSGTt2rBQUFNT5mJKSErnpppskOTlZPvzwQxNw3n77bWnfvn3VNosXL5bHHntMXnjhBdm+fbv069dPxo0bJxkZGWK19ucrLOm5RVJWXmF1cwAAcEs2+xUs4Xr69GlTadEgM3LkyFq3eeutt+Q3v/mNHDhwQLy9vWvdRisqWrn585//bL6uqKiQ+Ph4efTRR+UXv/jFZduRm5sroaGhkpOTIyEhIdKUKirs0uO5T6W03C5f/eKGqgADAACuTEOO31c0hkW/gQoPD69zm3//+98ybNgwU5mJioqSxMREeeWVV6S8vLyqArNt2zYZM2bM943y8DBfb9q0qdbnLC4uNi+y+qW5eHjYJKZq4C3dQgAAWKHRgUWrIDNnzpThw4ebEFKXI0eOmK4gDSg6buW5556T3/3ud/Lyyy+b+8+cOWPu0zBTnX6dnp5e51gaTWSOi1ZjmlNMKONYAABwycCiFZM9e/bIokWLLhtstNvof/7nf2TQoEFyxx13yLPPPmu6ihpr1qxZprrjuKSmpkpzcnQDMbUZAABreDXmQTNmzJDly5fL+vXrJS4u7pLb6swgHbvi6elZ9X+9evUy1RPtDoqIiDD3nTp1qsbj9Ovo6Ohan1NnGumlpbA8PwAALlRh0fG5GlaWLl0qq1evloSEhMs+RruMkpKSTKXF4bvvvjNBxsfHx1y08rJq1aqq+3Vb/VrHvjgDAgsAAC4UWLQb6L333pOFCxeatVi0SqKXwsLvD+STJ082XTYO06ZNk7Nnz8rPfvYzE1Q+/vhjM+hWn8tBpzTrVOd3331X9u/fbx6jU6Xvv/9+caa1WFieHwAAF+gSmjt3rrkePXp0jf+fN2+eTJkyxdxOSUkxs3wcdEDsZ599Jj//+c+lb9++Zv0VDS9PP/101TY6rkWnSD///PMmAPXv319WrFhx0UBcqzjGsKTlMIYFAACXW4fFWTTnOiwqv7hMEl/4zNze8+I4CfJt1NAfAABgxTos7kIDSohfZUjhnEIAALQ8AksDB94yjgUAgJZHYKkn1mIBAMA6BJZ6YmozAADWIbDUU8z5qc0ncwgsAAC0NAJLg7uECCwAALQ0AkuDu4QYwwIAQEsjsDQwsKTlFEpFhcsvXQMAgEshsNRTVLCveHrYpLTcLqfzi61uDgAAboXAUk9enh4SE1o58Db17DmrmwMAgFshsDRAfJsAc52aRWABAKAlEVgaID68chxL6llmCgEA0JIILI2psNAlBABAiyKwNEB8OF1CAABYgcDSAHQJAQBgDQJLI7qEdC2W0vIKq5sDAIDbILA0QLtgX/H18hBdN44l+gEAaDkElgaw2WwS14ZuIQAAWhqBpYEYeAsAQMsjsDRQB0dgYWozAAAthsDS6NVu6RICAKClEFgaPbWZCgsAAC2FwNJAcecrLMcZwwIAQIshsDRy0O2Z/BI5V1JmdXMAAHALBJYGCvX3lhA/L3P7OONYAABoEQSWK5nazDgWAABaBIGlEThrMwAALYvAcgUzhVJY7RYAgBZBYLmSxeOYKQQAQIsgsDRCHGNYAABoUQSWKxjDorOE7Ha71c0BAKDVI7A0guOMzfnFZZJ9rtTq5gAA0OoRWBrBz9tTIoN9zW3GsQAA0PwILFe8FgszhQAAaG4ElkaKP98tRIUFAIDmR2BpJFa7BQCg5RBYrnS1W84nBABAsyOwNFLc+dVuqbAAAND8CCxXuNrtiaxCqahgLRYAAJoTgaWRYkL9xcvDJiXlFXIqr8jq5gAA0KoRWBrJ08MmsWGObiHGsQAA0JwILE1w1mbGsQAA0LwILE0yU4jAAgCA0wSWOXPmyJAhQyQ4OFgiIyNl0qRJcvDgwUs+Zv78+WKz2Wpc/Pz8amwzZcqUi7YZP368uMpaLClUWAAAaFZeDdl43bp1Mn36dBNaysrK5JlnnpGxY8fKvn37JDAwsM7HhYSE1Ag2GkgupAFl3rx5VV/7+laeq8eZdWpb+ZqPZRJYAABwmsCyYsWKi6onWmnZtm2bjBw5ss7HaUCJjo6+5HNrQLncNs6mY9vKCsuxzAKrmwIAQKt2RWNYcnJyzHV4ePglt8vPz5eOHTtKfHy8TJw4Ufbu3XvRNmvXrjXhp0ePHjJt2jTJzMys8/mKi4slNze3xsXKwHImv0TyikotaQMAAO6g0YGloqJCZs6cKcOHD5fExMQ6t9MA8ve//12WLVsm7733nnnctddeK8ePH6/RHbRgwQJZtWqVvPrqq6bracKECVJeXl7nWJrQ0NCqiwYhKwT7eUtEkI+5TbcQAADNx2a32xu1TKtWQT799FPZsGGDxMXF1ftxpaWl0qtXL7nrrrvkpZdeqnWbI0eOSJcuXeSLL76QG2+8sdYKi14ctMKioUUrPjpepiXdNnejbDuWJW/+ZKDc3DemRb83AACuTI/fWnioz/G7URWWGTNmyPLly2XNmjUNCivK29tbBgwYIElJSXVu07lzZ4mIiKhzGx3voi+s+sUqjm6hZMaxAADQbBoUWLQYo2Fl6dKlsnr1aklISGjwN9Runt27d0tMTN3VCO0u0jEsl9rG2WYKJZ8hsAAA4BSBRac06ziUhQsXmrVY0tPTzaWw8Pul6SdPniyzZs2q+nr27Nny+eefm26e7du3yz333CPHjh2Tn/70p1UDcp988knZvHmzJCcnm3EsOjC3a9euMm7cOHGdmUKMYQEAwCmmNc+dO9dcjx49usb/6/opuvibSklJEQ+P73NQVlaWPPTQQybYtGnTRgYNGiQbN26U3r17m/s9PT1l165d8u6770p2drbExsaatV10fIsrrcVClxAAAE446NZVB+00tZxzpdJv9ufm9r7Z4yTAp0EZEAAAt5Xb3INu8b3QAG8JC/A2t+kWAgCgeRBYmkDHqiX66RYCAKA5EFiaQKeqqc1UWAAAaA4ElibA1GYAAJoXgaUJdIpg8TgAAJoTgaVJx7DQJQQAQHMgsDRhl1BaTpEUldZ+wkYAANB4BJYm0CbAW4L9KtdfSTlLlQUAgKZGYGkCNpuNgbcAADQjAksT4ZxCAAA0HwJLE3FUWI5QYQEAoMkRWJpI53aVgeXw6XyrmwIAQKtDYGki3SKDzfXhDAILAABNjcDSRLpEVlZYMgtK5GxBidXNAQCgVSGwNJEAHy9pH+ZvbidRZQEAoEkRWJpQ18ggc30oI8/qpgAA0KoQWJpQt/OBhQoLAABNi8DSDBUWAgsAAE2LwNKEukURWAAAaA4ElibUtV1w1UkQ84pKrW4OAACtBoGlCYUGeEvbQB9zmyX6AQBoOgSWJtbh/DmFOGszAABNh8DSxDqGcxJEAACaGoGliXU4fxJEKiwAADQdAksT63C+wpJylrM2AwDQVAgsTawjY1gAAGhyBJZmqrCczC6S0vIKq5sDAECrQGBpYpHBvuLn7SHlFXY5mV1odXMAAGgVCCxNzGazVVVZmCkEAEDTILA068BbAgsAAE2BwNIMOoQztRkAgKZEYGkGCRGVFRZOgggAQNMgsDSD3rEh5nrvyRyrmwIAQKtAYGkGPaNDxGYTOZVbLGfyi61uDgAALo/A0gwCfb0k4fwS/XtP5lrdHAAAXB6BpZm7hfYRWAAAuGIElmZyVWyouWYcCwAAV47A0kyosAAA0HQILM3kqvOB5WhmgeQXl1ndHAAAXBqBpZlEBPlKu2BfsdtFDrMeCwAAV4TA0ow6R1TOFDpyhsACAMCVILA0o87tKgPL0dMFVjcFAACXRmBpRp0jgsz14TMEFgAArgSBpQUqLEeosAAA0HKBZc6cOTJkyBAJDg6WyMhImTRpkhw8ePCSj5k/f77YbLYaFz8/vxrb2O12ef755yUmJkb8/f1lzJgxcujQIXF1CefHsCSfKZCKCrvVzQEAwD0Cy7p162T69OmyefNmWblypZSWlsrYsWOloODSFYSQkBBJS0uruhw7dqzG/a+99pq88cYb8tZbb8mWLVskMDBQxo0bJ0VFReLK4sMDxMvDJoWl5ZKe69qvBQAAK3k1ZOMVK1ZcVD3RSsu2bdtk5MiRdT5OqyrR0dG13qfVlT/+8Y/yy1/+UiZOnGj+b8GCBRIVFSUfffSR3HnnneKqvD09pEN4gBw5U2C6hWLD/K1uEgAA7jeGJSenctn58PDwS26Xn58vHTt2lPj4eBNK9u7dW3Xf0aNHJT093XQDOYSGhsrQoUNl06ZNtT5fcXGx5Obm1rg4/UwhpjYDANDygaWiokJmzpwpw4cPl8TExDq369Gjh/z973+XZcuWyXvvvWced+2118rx48fN/RpWlFZUqtOvHffVNpZGQ43jokHIWXVud36mEANvAQBo+cCiY1n27NkjixYtuuR2w4YNk8mTJ0v//v1l1KhRsmTJEmnXrp389a9/bey3llmzZpnqjuOSmpoqzqrr+cByKCPP6qYAAOAeY1gcZsyYIcuXL5f169dLXFxcgx7r7e0tAwYMkKSkJPO1Y2zLqVOnzCwhB/1aQ05tfH19zcUV9Ir5/iSIOl5Hx/MAAIBmrLDoAVfDytKlS2X16tWSkJDQwG8nUl5eLrt3764KJ/ocGlpWrVpVtY2OSdHZQlqdcXXdooLE08MmWedKmSkEAEBLBBbtBtJxKAsXLjRrsegYE70UFhZWbaPdP9pl4zB79mz5/PPP5ciRI7J9+3a55557zLTmn/70p+Z+rTjoWJiXX35Z/v3vf5swo88RGxtr1nlxdX7enlXdQlplAQAAzdwlNHfuXHM9evToGv8/b948mTJlirmdkpIiHh7f56CsrCx56KGHTLBp06aNDBo0SDZu3Ci9e/eu2uapp54ya7lMnTpVsrOzZcSIEWYK9YULzLmq3rEhcvBUnuxPy5Ube9UcXAwAAC7PZtd+HhenXUg6W0gH4Ooidc7mf9Yfllc+OSA/6BMtf7l7kNXNAQDA5Y7fnEuoBfSOCTXXdAkBANA4BJYW0Csm2FwnZ56T/OIyq5sDAIDLIbC0gLZBvhIVUjkN+2A6VRYAABqKwNJCeldbjwUAADQMgaUFZwqpfWkEFgAAGorA0tIDb9NYoh8AgIYisLTwwNsDablSVl5hdXMAAHApBJYW0rFtoAT4eEpxWYUkZ3LmZgAAGoLA0kL0fEI9oyurLHsZeAsAQIMQWCw4c/N+xrEAANAgBJYWdFVs5cDbPSdyrG4KAAAuhcDSgvrFVwaWb1OzpaLC5U/hBABAiyGwtKAeUcHi5+0hecVlcuRMvtXNAQDAZRBYWpCXp4f0aV9ZZdmRkm11cwAAcBkElhbWPz7MXO9MJbAAAFBfBJYW1j++jbkmsAAAUH8ElhbWv0NlheVAep4UlpRb3RwAAFwCgaWFxYb6SbtgXymvsMvek0xvBgCgPggsLcxmszGOBQCABiKwWMARWHYQWAAAqBcCiwUGOCosTG0GAKBeCCwW6BMXKjabyInsQjmdV2x1cwAAcHoEFgsE+3lL13ZB5jbjWAAAuDwCi0W+H3ibZXVTAABwegQWi9dj2XWcqc0AAFwOgcUiV8VWnlNo38lcsds5czMAAJdCYLHwzM0eNpHMghLJYOAtAACXRGCxiL+Pp3Q5P/BWqywAAKBuBBYL9Y4NMdf70ggsAABcCoHFQr1jzgcWKiwAAFwSgcUJBt5yEkQAAC6NwGKhXjHB5jo585zkF5dZ3RwAAJwWgcVCbYN8JSbUz9zee4IqCwAAdSGwWKxP+8puIRaQAwCgbgQWi/U7v0T/t8c5pxAAAHUhsDjJOYUILAAA1I3AYrHE811CqWcLJTOfFW8BAKgNgcViof7e0rldoLm9i4G3AADUisDiBPrFne8WSqVbCACA2hBYnEC/uMpuoZ0EFgAAakVgcQKDOoab623HsqSiwm51cwAAcDoEFidZ8TbAx1Pyisrku4w8q5sDAIDTIbA4AS9PDxnQoXIcyzfJWVY3BwAA1w4sc+bMkSFDhkhwcLBERkbKpEmT5ODBg/V+/KJFi8Rms5nHVTdlyhTz/9Uv48ePF3cy2NEtlHzW6qYAAODagWXdunUyffp02bx5s6xcuVJKS0tl7NixUlBQcNnHJicnyxNPPCHXXXddrfdrQElLS6u6vP/+++JOhnSqDCxUWAAAuJiXNMCKFStqfD1//nxTadm2bZuMHDmyzseVl5fL3XffLS+++KJ8+eWXkp198WwYX19fiY6OFnfVv0OYeHrY5ER2oaTlFEpMqL/VTQIAoHWMYcnJqVzoLDy8sjpQl9mzZ5tg8+CDD9a5zdq1a802PXr0kGnTpklmZmad2xYXF0tubm6Ni6sL8vUyg2/VVqosAAA0TWCpqKiQmTNnyvDhwyUxMbHO7TZs2CDvvPOOvP3223Vuo91BCxYskFWrVsmrr75qup4mTJhgKjN1jaUJDQ2tusTHx0trGseylXEsAAA0vkuoOh3LsmfPHhNI6pKXlyf33nuvCSsRERF1bnfnnXdW3e7Tp4/07dtXunTpYqouN95440Xbz5o1Sx577LGqr7XC0hpCi45jmb8xmXEsAAA0RWCZMWOGLF++XNavXy9xcXF1bnf48GEz2PaWW26pUZkx39jLy8ww0mByoc6dO5uAk5SUVGtg0fEuemltBndqY64PpOdKXlGpBPt5W90kAABcL7DY7XZ59NFHZenSpab6kZCQcMnte/bsKbt3767xf7/85S9N5eX111+vsypy/PhxM4YlJiZG3ElUiJ/Eh/ubMzfvSMmWkd3bWd0kAABcL7BoN9DChQtl2bJlZi2W9PR08/86jsTfv3JWy+TJk6V9+/ZmnImfn99F41vCwioXSHP8f35+vpk9dNttt5lZQlqVeeqpp6Rr164ybtw4cTdDOoZL6tkTZhwLgQUAgEYMup07d66ZGTR69GhT/XBcFi9eXLVNSkqKWUelvjw9PWXXrl1y6623Svfu3c1MokGDBpnpz62x2+dyBrMeCwAAF7HZtZ/HxemgW63yaJgKCQkRV3boVJ7c9If14uftIbteGCc+Xpw9AQDQOjXk+M3R0Ml0jQySNgHeUlRaIXtOVq5zAwCAuyOwOBk9j5Jjmf6vj7IeCwAAisDihK5OOD+OhcACAIBBYHFCjgrL1mNZUlHh8kOMAAC4YgQWJ3RVbIgE+HhKTmGpfJeRZ3VzAACwHIHFCXl5esjADpWr3tItBAAAgcXpu4W2EFgAACCwOP3A2+Sz5pQIAAC4MwKLkxrQIUy8PW1yKrfYnFsIAAB3RmBxUn7entKnfai5/XUy3UIAAPdGYHFiQ853C319NNPqpgAAYCkCixO7JqGtud54OJNxLAAAt0ZgcfKBtzqO5XhWoRzLPGd1cwAAsAyBxYkF+nrJgPPrsXyZdMbq5gAAYBkCi5O7rmuEuf7qEIEFAOC+CCxObkS3ysCy8fAZKee8QgAAN0VgcXJ948IkxM9LcovKZGdqttXNAQDAEgQWJ+fpYZNRPSLN7S/2n7K6OQAAWILA4gLG9KoMLCv3EVgAAO6JwOICRveIFC8PmyRl5MvRMwVWNwcAgBZHYHEBof7eck3nykXkVu5Lt7o5AAC0OAKLi7ipd5S5/mJfhtVNAQCgxRFYXMT15wfebkvJktyiUqubAwBAiyKwuIgObQOkc0SgWYuFReQAAO6GwOJCRvVoZ67XfXfa6qYAANCiCCwuZFT3ysCy9uBpzt4MAHArBBYXojOFfL08JD23SL47lW91cwAAaDEEFhfi5+0pw7pUTm9e9x2zhQAA7oPA4sLdQgAAuAsCiwuuequ+ST4rBcVlVjcHAIAWQWBxMZ3aBkiH8AApLbfLxsOZVjcHAIAWQWBxMTabTUZXTW9mHAsAwD0QWFwQ05sBAO6GwOKCdKaQj6eHHM8qlCOcvRkA4AYILC4owMdLrk4IN7eZLQQAcAcEFhf1/TgWAgsAoPUjsLj4OJbNRzKlsKTc6uYAANCsCCwuqmtkkLQP85eSsgpZfYDZQgCA1o3A4sLTm380sL25PX/jUaubAwBAsyKwuLB7rukoXh42+SY5S3Yfz7G6OQAANBsCiwuLCvGT/+obY27Po8oCAGjFCCwu7u5rOprrVfszpKy8wurmAADQLAgsLm5AfJiE+HlJTmGpfEu3EACglWpQYJkzZ44MGTJEgoODJTIyUiZNmiQHDx6s9+MXLVpkBovq46rT5eWff/55iYmJEX9/fxkzZowcOnSoIU1zW16eHjKiW4S5vZ41WQAArVSDAsu6detk+vTpsnnzZlm5cqWUlpbK2LFjpaDg8svDJycnyxNPPCHXXXfdRfe99tpr8sYbb8hbb70lW7ZskcDAQBk3bpwUFRU17NW4+ZosLCIHAGitbPYrOHve6dOnTaVFg8zIkSPr3K68vNzc/8ADD8iXX34p2dnZ8tFHH5n79NvHxsbK448/bgKNysnJkaioKJk/f77ceeedl21Hbm6uhIaGmseFhISIu0nLKZRhc1aLh01k2y9vkjaBPlY3CQCAJj1+X9EYFv0GKjy88rw2dZk9e7YJNg8++OBF9x09elTS09NNN5CDNn7o0KGyadOmWp+vuLjYvMjqF3cWE+ov3aOCpMIusnLfKaubAwBAk2t0YKmoqJCZM2fK8OHDJTExsc7tNmzYIO+88468/fbbtd6vYUVpRaU6/dpxX21jaTTUOC7x8fHi7iYNqFxEbvHWVKubAgCA8wQWHcuyZ88eM5C2Lnl5eXLvvfeasBIRUTkwtCnMmjXLVHccl9RUDtK3D4wTTw+bbDuWJUkZeVY3BwCAJuXVmAfNmDFDli9fLuvXr5e4uLg6tzt8+LAZbHvLLbfUqMyYb+zlZWYYRUdHm69PnTplZgk56Nf9+/ev9Xl9fX3NBd+LDPGT63tEyhf7T8nib1Ll2Zt7W90kAACsqbDoAFkNK0uXLpXVq1dLQkLCJbfv2bOn7N69W3bu3Fl1ufXWW+X66683t7UrR59DQ8uqVauqHqdjUnS20LBhwxr/ytzQHUMqu8aW7TwpFTqgBQAAd6ywaDfQwoULZdmyZWYtFscYEx1HouunqMmTJ0v79u3NOBM/P7+LxreEhYWZ6+r/r2NhXn75ZenWrZsJMM8995yZOXThei24tJHdIyTYz0sy8ople0qWDO506cHQAAC0ysAyd+5ccz169Oga/z9v3jyZMmWKuZ2SkiIeHg0bGvPUU0+ZtVymTp1qpjyPGDFCVqxYYQIP6s/Xy1Nu6hUlS3ackI93pxFYAACtxhWtw+Is3H0dlup0WvNDC7ZKTKiffPX0DeKhi7MAAODO67DA+VzXLUICfTwlLadItqVkWd0cAACaBIGllfHz9pQJfSpnWy3YdMzq5gAA0CQILK3QlGs7metPdqeZZfsBAHB1BJZWKLF9qFydEC7lFXaqLACAVoHA0ko9MLyyyvLhtuMmuAAA4MoILK3UDT2jJMTPS07nFcs3yWetbg4AAFeEwNJK+Xh5yLirKk978PGuNKubAwDAFSGwtGI3962cLfTpnjS6hQAALo3A0ooN7xohof7ecia/RLYczbS6OQAANBqBpRXz9vSQ8XQLAQBaAQJLK/df/Sq7hVbsSZey8gqrmwMAQKMQWFq5YZ3bSpsAb8ksKJHNR5gtBABwTQSWVs5Lu4USK6ssy3edtLo5AAA0CoHFDdxyvltoyfYTcvh0vtXNAQCgwQgsbtItNKp7Oykpr5Bnl+4Wu50pzgAA10JgcQM2m01enpQo/t6eZhzLp3vSrW4SAAANQmBxE/HhAfLQdQnm9t++PGJ1cwAAaBACixu5d1gn8fH0kO0p2bI9Jcvq5gAAUG8EFjfSLthXJvaPNbff2XDU6uYAAFBvBBY38+D5bqFPd6fJ8axzVjcHAIB6IbC4mZ7RITKia4TouRDf3ZhsdXMAAKgXAosbenBEZZVl0depkldUanVzAAC4LAKLG9I1Wbq0C5S84jL5zWcHrW4OAACXRWBxQx4eNnnhlqvM7QWbjsnne1mXBQDg3Agsbmpk93YydWRnc/u5ZXukpIwzOQMAnBeBxY09Pra7RIX4yqncYlm284TVzQEAoE4EFjfm6+UpDwyvHID71/VHpEKnDgEA4IQILG7urqEdJNjXS5Iy8mX1gQyrmwMAQK0ILG4uxM9bfnJNB3P7r+sPW90cAABqRWCB6RbScwx9k5wl246dtbo5AABchMACiQrxkx8OaG9u/3l1ktjtjGUBADgXAguMh0Z2Fk8Pm6w5eFr+dzszhgAAzoXAAqNrZJA8dlN3c/uFZXskJZMTIwIAnAeBBVUeHtVFrk4Il4KScvn1iv1WNwcAgCoEFlTRLqGXJiaKh03kk93psj0ly+omAQBgEFhQQ4/oYLl9UJy5/cKyvVJYUm51kwAAILDgYo+P7SGh/t6y+0SOPPLPbVJaznmGAADWIrCg1mnOf58yWPy8PcysoX9tTbW6SQAAN0dgQa0GdQyXn4+pnDX00Q6mOQMArEVgQZ0m9m8vNpuYFXCPZzHNGQBgHQIL6hQd6ifXJLQ1t//zbZrVzQEAuDECCy5pYv9Yc/3e5mOSepYqCwDAGgQWXNIP+sZI+zB/OZFdKD/8y1dy6FSe1U0CALghAgsuKcTPW/532rXSOyZEzuSXyCP/3M7aLAAA5w4sc+bMkSFDhkhwcLBERkbKpEmT5ODBg5d8zJIlS2Tw4MESFhYmgYGB0r9/f/nHP/5RY5spU6aIzWarcRk/fnzjXhGaZSzLuw9cLe2CfeVQRr7MXr7P6iYBANxMgwLLunXrZPr06bJ582ZZuXKllJaWytixY6WgoKDOx4SHh8uzzz4rmzZtkl27dsn9999vLp999lmN7TSgpKWlVV3ef//9xr8qNDkNK3+8o7+5/f7XKSzbDwBoUTa73W5v7INPnz5tKi0aZEaOHFnvxw0cOFBuvvlmeemll6oqLNnZ2fLRRx81qh25ubkSGhoqOTk5EhIS0qjnQP088cG38uG249I/PkyWTLtWPPTEQwAANPPx+4rGsOg3cFRR6kOz0apVq0w30oUBZ+3atSb89OjRQ6ZNmyaZmZl1Pk9xcbF5kdUvaBlPjeshgT6esjM1W5Z9y4JyAICW0ejAUlFRITNnzpThw4dLYmLiZYNNUFCQ+Pj4mMrKn/70J7nppptqdActWLDAhJlXX33VVGwmTJgg5eXldY6l0UTmuMTHxzf2ZaCBIkP85JHru5rbr356UM6VlFndJACAG2h0l5BWQT799FPZsGGDxMVVnt33UuHmyJEjkp+fb0KJdgVp98/o0aNr3V637dKli3zxxRdy44031lph0YuDVlg0tNAl1DKKSsvlpj+sk9SzhTLj+q7yxLgeVjcJAOCCmr1LaMaMGbJ8+XJZs2bNZcOK+SYeHtK1a1czQ+jxxx+X22+/3VRJ6tK5c2eJiIiQpKSkWu/39fU1L6z6BS3Hz9tTnpnQy9x+c22S/GPzMaubBABo5RoUWLQYo2Fl6dKlsnr1aklISGjUN9WKS/UKyYWOHz9uxrDExMQ06vnR/MYnRst9wzqK1uee+2iPfMAZnQEAzhJYdErze++9JwsXLjRrsaSnp5tLYWFh1TaTJ0+WWbNmVX2tlRSdAq3dPPv375ff/e53Zh2We+65x9yv3URPPvmkmSqdnJxsuowmTpxoKjLjxo1ryteKJqRr5fzq1qvk/47qbL5+ftleScpgFVwAQPPwasjGc+fONdcXjj2ZN2+emZqsUlJSTBeQg67R8sgjj5iqib+/v/Ts2dOEnjvuuMPc7+npadZneffdd83U5tjYWLO2i45z0a4fOHdoeXpcT9l7Ilc2JJ2RGQt3yL9njBAfLxZQBgA40ToszoJ1WKyVkVckE/74pWQWlMijN3SVx8cyCBcA4ETrsAAqMthPXppUObX9L2sPy67j2VY3CQDQyhBY0CR+0CdGbu4bI+UVdpn23nbJKiixukkAgFaEwIIm88oP+0jHtgFyIrtQHnz3GwbhAgCaDIEFTSbU31veumeQBPh4yvaUbBn3xy/lt58dlJKyCqubBgBwcQQWNKleMSHy8X9fJ2N6RZnuoT+vSZJ7/rZFKipcfmw3AMBCBBY0uYSIQPnbfYPlzZ8MNCdK/Dr5rHy8O83qZgEAXBiBBc1GB+FOHdnF3H591SFTcQEAoDEILGhW94/oJCF+XpKUkS+jf7tGXlq+T4rLaj8LNwAAdSGwoFmF+HnLz2/qbm7r2Z3f2XBU7n3na8k5V2p10wAALoTAgmZ3//AE+ebZMfKnuwZIsK+XfH30rMxcvMOcTBMAgPogsKBFtAv2lVv6xcr7U68RXy8PWXPwtLy17gihBQBQLwQWtKjE9qHy3H/1NrdfXXFAfvL2Fkk9e87qZgEAnByBBS3u7qEd5L9v6Co+nh6y6Uim3D//G8ktYkwLAKBuBBa0OJvNJo+N7SFfPDZKokP8zAyiRxfuYNozAKBOBBZYpkPbALPAnJ+3h6z77rS88sl+q5sEAHBSBBZYPqbl9z/ub27rlOfHFu+UI6fzrW4WAMDJEFhguR/0iZEnx/Uwt5fsOCG3/GmDHCa0AACqIbDAKUy/vqt8NH249IsPk4KScvnZoh2SkVvESRMBAAaBBU6jf3yY/PWeQRIW4C17TuTK1a+sktG/XWvGtwAA3BuBBU4lOtRP/nzXQIkP9xebTSTl7Dm57+9fy2P/2ilZBSVWNw8AYBGbvRUsNZqbmyuhoaGSk5MjISEhVjcHTaSguEx+9/l3Mm/jUdF3aUSQj7x4a6L8oE+0mRoNAHCf4zcVFjitQF8vef6W3vLhw9dKt8ggOZNfItMXbpdf/O9uKS2vsLp5AIAWRGCB0xvUsY0s/+8RZnVcD5vI4q2pcu87W8xMoq3JZ2XPiRyrmwgAaGZ0CcGlrNp/Sh59f4ecKymv+j8NMR9Ou1YGdmhjadsAAA1DlxBarRt7Rcl/Hh0h13WLMF97ethEZz4/9eEuKS77PsQAAFoXKixwSfq2zSwoMYNxJ7y+3oxvCfbzMlOjbx8UZxaj8/YkjwOAM6PCglZPZwlFBPlKu2Bf+c3t/STQx1Pyisrky0Nn5GeLdsqtf/6KsS0A0IpQYUGrUFRaLkdOF8hne9NlwaZkyTpXav5fV84dEB8mvWKCTdUl2M/b6qYCABpx/CawoNU5k18ss/+zTz7enSbl1Zb29/f2lCEJ4TKqezu5b1hH8aLLCAAsRWABROR0XrGsPnBKjpwpkFX7MyQp4/sTKo7s3k7euLO/hAX4WNpGAHBnuQQWoCZ9m+89mSsbD5+RP6w8JIWl5RLs6yW3DYqTIZ3C5cZekeLn7Wl1MwHAreQSWIC66WDcx//1rRw8lVf1f71jQuSfPx0qpRUVEubvIz5edBcBQHMjsACXUVFhl9UHMmTNwQz5dE+6nC0oMSGlpKxCfL08ZFiXtvLypESJaxNgdVMBoNUisAANcOhUntz19hYzWLe68EAfueeajuakizp4V7uMOrUNlGs6h3PyRQBoAgQWoBEDdI9lFshVsaGSnFkgT374rew5kVvrtrcNjJNQf2+z/YhuEfLDAe0ZvAsAjUBgAZpgXZd/bkmRpIw8yT5Xak4BUFBcJuu+O21OBVBddIifvHZ7X/H38ZSEiECzoB0A4PIILEAz+fLQaXl+2V4TTAZ3aiMfbD0uR88UVN3v5WGTMb2i5IVbe0tMqL+lbQUAZ0dgAVpIXlGp/PKjPbJ6f4Y5l9HJnCLz/1pl+fHgOPGw2STA11OKSytMBeYnQztIyPnVdjXopGUXSptAH+kZHcy4GABuJ5fAAlhjf1qu/HzxTjmQ/v2U6eo6tg2Q6aO7ytZjZ+VfW49X/f+913SUZ2/uZQb+nsotkqgQP2YoAWj1cgksgHXOlZTJvK+SJSO3stqSX1wuft4esvbgaTmRXVhj2y7tAs1KvLV9CrtGBsn1PdqZ2Uk6pmbysE4SH14ZYvS5/Xw8q6o1SrfRKdlUagC4CgIL4ISyz5XIH784ZLqCvD09ZNrozjKoY7h8vCtNnvjgW7P6ro+nhzkDdXpuUY3zIKkQPy95cERnSc8tNNUZPy8PeWBEgoztHS1vrkmSFXvTJTbUz5zk8WdjulWd6FE/4sezCs007UBfL4tePQBcjMACuGBV5lxJuYQH+IiHh01yzpXKl0mnZcOhM6IFk31pefJtana9ny8y2Feuig2RkvIKOXq6wIytCfL1kt/+n74yPjHGnKLgvc3HzBmu2wb5yKT+7SWzoMRM19Zp2rWdpkAD14XTt/eezJEFG49JiL+XXNs1QkZ3b1evCo/j1w7VIMC95RJYgNZFV+BdsClZ9qflmYP9HUPiJetciZl6veXIWTM2Zs6P+pjQ8f8+3i8pZ8/V+VxawdF1Zy51/7RRXcTby0M+35suvWND5HBGvnyxP8PMgHrougTZeixLUjLPyZIdx6W0/PtfITf3iZHRPdqZrqtrOret0V31wdZU2ZB0RlLOFsrxs+dMtecfD14tXdoFSUFJWVVFqLrisnLZdDhT+rQPlbZBviY0aTeYhrr6mv/VUVmy44T86tarZGCHNpfcdntKlugz62tOyy6S2DB/TtPQyu1MzZbS8gpzTjFnOXXIruM55jOuyym0drnNFVjmzJkjS5YskQMHDoi/v79ce+218uqrr0qPHj3qfIxu/8orr0hSUpKUlpZKt27d5PHHH5d77723ahttwgsvvCBvv/22ZGdny/Dhw2Xu3Llm26Z+wUBro11H1X+xFZaUmwqKDuDVric96OoB+M3VSfK3DUfN9rq5zli6oWekbD+WLWu/y5D4NgHmF+WF42wuR59DKzofbjsuZdW6sSYkRpuQoFWcx/61U9LOz6CqLirEV7w8PMz31BWFp1/fVe4fnmDG6Ly3JUUWbjkmZ/JLJK6Nv9x1dQf5/crvpFtkkLxx1wDx9/Y0r/NY5jkzQDkjr8ic4PK7U3nSITxAbukXawYvT5n3tRkjFOjjKb+Y0FPaBfuZIKSP16nnPaKDzXT03608KG+uOXxR+965b4gktg+t2ref7E4TX28Pual3lPh6fV+J2nYsSwJ8PE3gm7pgq/mZvHhrotn3F4bPVftPycbDmaYy9cDwBFO50lBXbrdLdkGpJJ3Ok6Nnzsngjm2kb1yoqUTN++qoea239o+VfnFhtR7M9Hfpv7amylvrjpg23zawvfSPD5PcwjLx9LRJ+zD/qjZ8vPukfHcq31Te7h/eSb5KyjQHyynXdjIz1xzPl3q2UE7nF0n7sACzP7Rqt2zHSekSGSSDOraRnMJSOZ1XJCH+3hIZ7FdVeXv0/R2mYqg/N63aaTHtVG6xmS2nJx51hM4dKVny2d5Tpg06u+7w6Xyz5pH+XAJ8vEz18ZvkLDmYnmvez7q445BObRpdndPXpIFef/75xWUy/o9fSllFhfx7xgizzzRYv7XusHkf6krX1R+nXau6Dx1t35h0Rr46fMa8t9NziszP5NouEfJffWNM0NXPRGZ+iRnD1i8+TK7rFmFek9Iq6tHMAukVE1z1PsrML5Yxv18nWedK5fGbustDIzubQfj6Pb08Pcyq3I/8c7sJ3s/f0tucXsTb02b+QNA/ZmLD/GRoQtuq94buO32uvSdyJLeozHwmG9ItrL8rjp7Jl5zCMukXF2ra4DKBZfz48XLnnXfKkCFDpKysTJ555hnZs2eP7Nu3TwIDA2t9zNq1ayUrK0t69uwpPj4+snz5chNYPv74Yxk3bpzZRkOPhqF3331XEhIS5LnnnpPdu3eb5/Xz82vSFwy4+zRsPUhpwHAM4K1OD2T6S1bHxOgv8buHdpQD5w8UExJj5LUVB8z4mlHd25lBwfoLfmzvKHPw2HU8W/667ojkFpWaqoiGFx0ErL/09HZMqJ8ZONwjOkjaBfnJzMU75PDp79ewcdAgsO7gaXNgVHpcutI6sB4g84rLar1PDyaqqLTy++kBXA9kju+rB56u7YJMSEk+U2AOAErHBGkXmAYSDUpLd5wwj9GFBB3hTIOQBsPBncLl2JkCCfLzkve/TjE/Awef8/vowjFLDhpYtFr1P+uPVP2fHqS0MqVhRGekFZdVyNWd2si2lKw6V2hWQxPCZWDHNvLFvlNyKCO/1qpbmwBv+fHgeNHWLNt5woQMB/2Ze9psVScOjQ/3N4FG6THylR/2MWsUPbRgqzlAOkzsH2sOvJuPnK3aVvfTmN5RsvibVNN+DUNaPXO0S8OLjsX6w8rvzLm+qtN26HivCrvdhMS+cWHmZ7bvZK45aGtI1+5NvWjY7RQRaML45sOZJijqe1gDi7Z1X1rl/tJAkRgbYqqWDnqA1/2ij0/KyDfVGA1LGjIXfZNqFpKsjb4PvD08qt7DF57uQ1/7qysOmLCn7ddZgo+N7S6z/ne3qQY63jsaHPX76+fo5r4xplvY8ZnRx2k38oX0Z3n7oDgTBB3720H3hb4ftZqpAV7DU2ZBsazYky6BPl5yfc92JpQnnc437+kDaXlmbJ26OiFc/nTXAPNHgEt2CZ0+fVoiIyNl3bp1MnLkyHo/buDAgXLzzTfLSy+9ZFJrbGysCTFPPPGEuV8bHhUVJfPnzzcB6XIILEDT0s+l/ma4sOtFD6p63+X+0tK/sHWBPa06OA5Yr97Wt8bYmJPZhfLbzw6a0DNpQHv525dH5C9rv69w6F/v+ld3Ymyo3PX2ZhMCHhyRYKaO60FH/4rUA4w+Xp9Lf7nrtt2jguXb49my6JsUczDVX9JLHxluKhT6S1gDlZ+Xp5wrLTenV9CVjJX+AteK0I8GxpnqlLZ1xsLt8uWhMzVemz5fWbndHPSqqx6stFrUP76NfLH/VK37p22gj6kA6fT2CwOGBiitGOlf1ZuPZJqDefXAoQdex0GkNnpw00qVtk8XOtR9oKGorLyixirN2kYNoZ/tTZeM82FFX5tWEarTcKSVk+oDwXUAuB4sHRU1RyDUt4v+j+4HrQ5d162dvLH6UJ1hzEHDg+M16W1trx7MHfQArws1ateNhgRHuGwKGnz0NRZUO/gP79rWVJwuRx93a7/20iUy0ARyfS/pz9zxWN0HQxLCJaugxLyPLqxe6s/K8fN1BBB9H2kFxfHZ0X1a8+fmK/nFpWYf6O3wQG/TfdkzJtiEPcf72UGDjwYzDXS1VTkvR38edrGb76eB68OHh0nndkHicoFFu3m020arIYmJiZfdXr/V6tWr5dZbb5WPPvpIbrrpJjly5Ih06dJFduzYIf3796/adtSoUebr119//aLnKS4uNpfqLzg+Pp7AAjgR/bxrsNBfoD/oE33ZEr5ur7Oo9CA+dWQXU8Vx0PJ5ata5Gl0zemC/1HPqQVJ/6esva/2rs66zdmtZXqsG7dv4m0rShW3SaoIeEPSvZV0c8OrzYx2+PnpWvkw6Yw7wup2GK32ty3elycOjupguDa00/XX9YVOV0SpNTmGJdGwbKI/e0NV0A+n31y4QrbxoNUAXGqwe6jQ4zf7PPvn3tyflvmEdTaDSg1daTqF8m5oju05km+fVA/w3yWelR3SIqQpUPz2EHqgCvD1N4NAZaXrQ1MChM8y0DfoX/DsbjpqDtFZydJstRzPNwVNnnOnPQdukQW/ZjhOmO0V/PtqFpd1vWt3Q8DNryW5TdVD6F/7siVeZv+B1HNR/L9phAthf7x1sQpGGEd1/OvBbF02cOaa7/OGL76RNgI9pl/5YH3p3q2w5elZ+NLC9qdw49ot2m+hjdVq/VjG0QrAjJdtUwAbEtzFdg/qa9GeRXVhqxl/p+cH0++hZ2LXLRqtWL/5nn6kmPjmuh9kfzy3ba8ZKPT2+p9kXC79OMQPWzRiu0/ni4+lpzh32/z7eZ37mup+nDE8w768LaRAuKC433T2O96gGxv/sOimf7z1luklv6BUpM8d0kzUHTsusJbvMe0TDxeNje8hPru4gv/n8gKmiaZeaBu05n+w3lQ/totT3vo5du31wXI2lDTTQ6fNrZUyrTD+9LsHsd22Dvg/mbThq3u/nisvNuDHtetP33I29okz1VStI+j7s0DbAdL31jgkxr09fz/SFO0z17R8PDm3SsTUtElgqKipM8NAxJxs2bLjkttqQ9u3bm5Dh6ekpf/nLX+SBBx4w923cuNGMWTl58qTExMRUPebHP/6x2cmLFy++6Pl+9atfyYsvvljr9yGwAGht9GBS26BkZ6IB8cNtqZIQEWS6Dy5sv4aXhhzoNMxpMNBup+aaTaZdTVo1UFoFCQvwtmTmmgY4Xe6ge1RQ1RiXuvaJh0UDcTWg6h8KjvFNVgSWRi/KMH36dDN+5XJhRQUHB8vOnTslPz9fVq1aJY899ph07txZRo8e3ajvPWvWLPMcF1ZYAKA1cvawojSM3DGkQ5O1Xw/M+pd+c3KEFdXUB+KG0KqGjke6HA8LZw1phau25Q5aUqMCy4wZM8zg2fXr10tcXNxlt/fw8JCuXbua29rNs3//fjPIVgNLdHS0+f9Tp07VqLDo19W7iKrz9fU1FwAA4B4aNEdJe480rCxdutSMRdEZPY3tTnKMQdHn0NCilZfqFZMtW7bIsGHDGvX8AACgdfFqaDfQwoULZdmyZaabJz093fy/9j/puixq8uTJZryKVlCUXg8ePNgMrNWQ8sknn8g//vEPs86K0v7CmTNnyssvv2wG8DqmNevMoUmTJjX9KwYAAK07sDhCxoVjT+bNmydTpkwxt1NSUkwXkENBQYE88sgjcvz4cRNqdD2W9957T+64446qbZ566imz3dSpU80g3hEjRsiKFSvqtQYLAABo/ViaHwAAOP3xm5NkAAAAp0dgAQAATo/AAgAAnB6BBQAAOD0CCwAAcHoEFgAA4PQILAAAwOkRWAAAgNNr9NmanYlj7TtdgAYAALgGx3G7PmvYtorAkpeXZ67j4+OtbgoAAGjEcVxXvG31S/Pr2Z9PnjxpTsioJ1Ns6vSnQSg1NZVl/y+DfdUw7K/6Y181DPur/thX1u4rjSAaVvSEx9XPQ9hqKyz6IuPi4pr1e+gPhzdz/bCvGob9VX/sq4Zhf9Uf+8q6fXW5yooDg24BAIDTI7AAAACnR2C5DF9fX3nhhRfMNS6NfdUw7K/6Y181DPur/thXrrOvWsWgWwAA0LpRYQEAAE6PwAIAAJwegQUAADg9AgsAAHB6BJbLePPNN6VTp07i5+cnQ4cOla+//lrc3a9+9SuzonD1S8+ePavuLyoqkunTp0vbtm0lKChIbrvtNjl16pS4g/Xr18stt9xiVm3U/fLRRx/VuF/HuD///PMSExMj/v7+MmbMGDl06FCNbc6ePSt33323WZgpLCxMHnzwQcnPzxd33F9Tpky56L02fvx4t9xfc+bMkSFDhpgVvSMjI2XSpEly8ODBGtvU57OXkpIiN998swQEBJjnefLJJ6WsrEzcbV+NHj36ovfWww8/7Hb7au7cudK3b9+qxeCGDRsmn376qVO+pwgsl7B48WJ57LHHzDSu7du3S79+/WTcuHGSkZEh7u6qq66StLS0qsuGDRuq7vv5z38u//nPf+SDDz6QdevWmdMm/OhHPxJ3UFBQYN4nGnRr89prr8kbb7whb731lmzZskUCAwPNe0p/KTjowXfv3r2ycuVKWb58uTmoT506VdxxfykNKNXfa++//36N+91lf+lnSQ8cmzdvNq+1tLRUxo4da/ZhfT975eXl5sBSUlIiGzdulHfffVfmz59vQrS77Sv10EMP1Xhv6efT3fZVXFyc/PrXv5Zt27bJ1q1b5YYbbpCJEyeaz5TTvad0WjNqd/XVV9unT59e9XV5ebk9NjbWPmfOHLs7e+GFF+z9+vWr9b7s7Gy7t7e3/YMPPqj6v/379+vUefumTZvs7kRf89KlS6u+rqiosEdHR9t/85vf1Nhfvr6+9vfff998vW/fPvO4b775pmqbTz/91G6z2ewnTpywu9P+Uvfdd5994sSJdT7GnfdXRkaGee3r1q2r92fvk08+sXt4eNjT09Ortpk7d649JCTEXlxcbHeXfaVGjRpl/9nPflbnY9x1X6k2bdrY//a3vznde4oKSx00LWri1JJ99XMW6debNm0Sd6fdGFrG79y5s/kLV0uCSveZ/jVTfb9pd1GHDh3cfr8dPXpU0tPTa+wbPYeGdjU69o1ea7fG4MGDq7bR7fW9pxUZd7R27VpTZu7Ro4dMmzZNMjMzq+5z5/2Vk5NjrsPDw+v92dPrPn36SFRUVNU2WuHTk9o5/qJ2h33l8M9//lMiIiIkMTFRZs2aJefOnau6zx33VXl5uSxatMhUorRryNneU63i5IfN4cyZM+aHV/2HoPTrAwcOiDvTA6yW/PQAomXUF198Ua677jrZs2ePOSD7+PiYg8iF+03vc2eO11/be8pxn17rwbk6Ly8v84vWHfefdgdp+TkhIUEOHz4szzzzjEyYMMH8kvT09HTb/aVnqJ85c6YMHz7cHGxVfT57el3b+89xn7vsK/WTn/xEOnbsaP7w2rVrlzz99NNmnMuSJUvcbl/t3r3bBBTtmtZxKkuXLpXevXvLzp07neo9RWBBg+kBw0EHa2mA0Q/+v/71LzOQFGgqd955Z9Vt/StO329dunQxVZcbb7xR3JWOz9A/EKqPHUPD9lX1cU763tKB8Pqe0mCs7zF30qNHDxNOtBL14Ycfyn333WfGqzgbuoTqoGVC/QvuwtHQ+nV0dLRl7XJGmr67d+8uSUlJZt9od1p2dnaNbdhvUvX6L/We0usLB3XraHudCePu+09pF6R+NvW95q77a8aMGWZw8Zo1a8yASYf6fPb0urb3n+M+d9lXtdE/vFT195a77CsfHx/p2rWrDBo0yMyw0oHwr7/+utO9pwgsl/gB6g9v1apVNUqL+rWWzvA9nUKqf5XoXyi6z7y9vWvsNy2z6hgXd99v2q2hH+Dq+0b7eXWshWPf6LX+ctC+Y4fVq1eb957jF6o7O378uBnDou81d9tfOi5ZD8BartfXqO+n6urz2dNrLf9XD3k6i0ans2oXgLvsq9pohUFVf2+5w76qjX5+iouLne891aRDeFuZRYsWmRkc8+fPN7MRpk6dag8LC6sxGtodPf744/a1a9fajx49av/qq6/sY8aMsUdERJiR+Orhhx+2d+jQwb569Wr71q1b7cOGDTMXd5CXl2ffsWOHuejH6/e//725fezYMXP/r3/9a/MeWrZsmX3Xrl1mBkxCQoK9sLCw6jnGjx9vHzBggH3Lli32DRs22Lt162a/66677O62v/S+J554wsxG0PfaF198YR84cKDZH0VFRW63v6ZNm2YPDQ01n720tLSqy7lz56q2udxnr6yszJ6YmGgfO3asfefOnfYVK1bY27VrZ581a5bdnfZVUlKSffbs2WYf6XtLP4+dO3e2jxw50u321S9+8Qsze0r3g/5O0q91lt3nn3/udO8pAstl/OlPfzI/LB8fHzPNefPmzXZ3d8cdd9hjYmLMPmnfvr35Wn8BOOjB95FHHjFT4wICAuw//OEPzS8Ld7BmzRpz4L3wotNzHVObn3vuOXtUVJQJwzfeeKP94MGDNZ4jMzPTHHCDgoLM1MD777/fHLzdbX/pwUV/CeovP51a2bFjR/tDDz100R8M7rK/attPepk3b16DPnvJycn2CRMm2P39/c0fGvoHSGlpqd2d9lVKSooJJ+Hh4eZz2LVrV/uTTz5pz8nJcbt99cADD5jPlv4+18+a/k5yhBVne0/Z9J+mrdkAAAA0LcawAAAAp0dgAQAATo/AAgAAnB6BBQAAOD0CCwAAcHoEFgAA4PQILAAAwOkRWAAAgNMjsAAAAKdHYAEAAE6PwAIAAJwegQUAAIiz+//dnU5SK+7j+gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Epoch 0, loss: 2.301800\n",
      "Epoch 1, loss: 2.302558\n",
      "Epoch 2, loss: 2.302208\n",
      "Epoch 3, loss: 2.303309\n",
      "Epoch 4, loss: 2.302126\n",
      "Epoch 5, loss: 2.301524\n",
      "Epoch 6, loss: 2.301150\n",
      "Epoch 7, loss: 2.301359\n",
      "Epoch 8, loss: 2.302305\n",
      "Epoch 9, loss: 2.302205\n",
      "Epoch 10, loss: 2.301595\n",
      "Epoch 11, loss: 2.302039\n",
      "Epoch 12, loss: 2.301247\n",
      "Epoch 13, loss: 2.301836\n",
      "Epoch 14, loss: 2.302396\n",
      "Epoch 15, loss: 2.302299\n",
      "Epoch 16, loss: 2.301161\n",
      "Epoch 17, loss: 2.302058\n",
      "Epoch 18, loss: 2.302180\n",
      "Epoch 19, loss: 2.302339\n",
      "Epoch 20, loss: 2.302694\n",
      "Epoch 21, loss: 2.301872\n",
      "Epoch 22, loss: 2.302261\n",
      "Epoch 23, loss: 2.302209\n",
      "Epoch 24, loss: 2.302215\n",
      "Epoch 25, loss: 2.301591\n",
      "Epoch 26, loss: 2.301535\n",
      "Epoch 27, loss: 2.302557\n",
      "Epoch 28, loss: 2.301689\n",
      "Epoch 29, loss: 2.301815\n",
      "Epoch 30, loss: 2.301483\n",
      "Epoch 31, loss: 2.301553\n",
      "Epoch 32, loss: 2.301640\n",
      "Epoch 33, loss: 2.301991\n",
      "Epoch 34, loss: 2.302104\n",
      "Epoch 35, loss: 2.301741\n",
      "Epoch 36, loss: 2.302652\n",
      "Epoch 37, loss: 2.303017\n",
      "Epoch 38, loss: 2.301521\n",
      "Epoch 39, loss: 2.301504\n",
      "Epoch 40, loss: 2.301565\n",
      "Epoch 41, loss: 2.302361\n",
      "Epoch 42, loss: 2.302223\n",
      "Epoch 43, loss: 2.302385\n",
      "Epoch 44, loss: 2.302532\n",
      "Epoch 45, loss: 2.301950\n",
      "Epoch 46, loss: 2.301686\n",
      "Epoch 47, loss: 2.301881\n",
      "Epoch 48, loss: 2.302291\n",
      "Epoch 49, loss: 2.302069\n",
      "Epoch 50, loss: 2.301971\n",
      "Epoch 51, loss: 2.302533\n",
      "Epoch 52, loss: 2.302043\n",
      "Epoch 53, loss: 2.302003\n",
      "Epoch 54, loss: 2.301729\n",
      "Epoch 55, loss: 2.301272\n",
      "Epoch 56, loss: 2.302220\n",
      "Epoch 57, loss: 2.301765\n",
      "Epoch 58, loss: 2.301959\n",
      "Epoch 59, loss: 2.302229\n",
      "Epoch 60, loss: 2.301372\n",
      "Epoch 61, loss: 2.302540\n",
      "Epoch 62, loss: 2.301929\n",
      "Epoch 63, loss: 2.301502\n",
      "Epoch 64, loss: 2.302201\n",
      "Epoch 65, loss: 2.302674\n",
      "Epoch 66, loss: 2.302191\n",
      "Epoch 67, loss: 2.302989\n",
      "Epoch 68, loss: 2.301388\n",
      "Epoch 69, loss: 2.301621\n",
      "Epoch 70, loss: 2.301533\n",
      "Epoch 71, loss: 2.302804\n",
      "Epoch 72, loss: 2.301514\n",
      "Epoch 73, loss: 2.300962\n",
      "Epoch 74, loss: 2.302366\n",
      "Epoch 75, loss: 2.301895\n",
      "Epoch 76, loss: 2.301768\n",
      "Epoch 77, loss: 2.302649\n",
      "Epoch 78, loss: 2.302394\n",
      "Epoch 79, loss: 2.301415\n",
      "Epoch 80, loss: 2.302148\n",
      "Epoch 81, loss: 2.302263\n",
      "Epoch 82, loss: 2.301172\n",
      "Epoch 83, loss: 2.302803\n",
      "Epoch 84, loss: 2.301892\n",
      "Epoch 85, loss: 2.302788\n",
      "Epoch 86, loss: 2.302016\n",
      "Epoch 87, loss: 2.302450\n",
      "Epoch 88, loss: 2.302002\n",
      "Epoch 89, loss: 2.302363\n",
      "Epoch 90, loss: 2.301257\n",
      "Epoch 91, loss: 2.302553\n",
      "Epoch 92, loss: 2.301951\n",
      "Epoch 93, loss: 2.300910\n",
      "Epoch 94, loss: 2.302318\n",
      "Epoch 95, loss: 2.301563\n",
      "Epoch 96, loss: 2.302494\n",
      "Epoch 97, loss: 2.301579\n",
      "Epoch 98, loss: 2.302468\n",
      "Epoch 99, loss: 2.302352\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.301406\n",
      "Epoch 1, loss: 2.300645\n",
      "Epoch 2, loss: 2.301930\n",
      "Epoch 3, loss: 2.300194\n",
      "Epoch 4, loss: 2.296883\n",
      "Epoch 5, loss: 2.297291\n",
      "Epoch 6, loss: 2.293188\n",
      "Epoch 7, loss: 2.297095\n",
      "Epoch 8, loss: 2.290985\n",
      "Epoch 9, loss: 2.289429\n",
      "Epoch 10, loss: 2.289858\n",
      "Epoch 11, loss: 2.289787\n",
      "Epoch 12, loss: 2.291270\n",
      "Epoch 13, loss: 2.294006\n",
      "Epoch 14, loss: 2.290483\n",
      "Epoch 15, loss: 2.281833\n",
      "Epoch 16, loss: 2.288653\n",
      "Epoch 17, loss: 2.280844\n",
      "Epoch 18, loss: 2.286164\n",
      "Epoch 19, loss: 2.283487\n",
      "Epoch 20, loss: 2.280167\n",
      "Epoch 21, loss: 2.281047\n",
      "Epoch 22, loss: 2.282983\n",
      "Epoch 23, loss: 2.283910\n",
      "Epoch 24, loss: 2.285083\n",
      "Epoch 25, loss: 2.279782\n",
      "Epoch 26, loss: 2.276024\n",
      "Epoch 27, loss: 2.280392\n",
      "Epoch 28, loss: 2.280157\n",
      "Epoch 29, loss: 2.286447\n",
      "Epoch 30, loss: 2.276390\n",
      "Epoch 31, loss: 2.277262\n",
      "Epoch 32, loss: 2.271290\n",
      "Epoch 33, loss: 2.273131\n",
      "Epoch 34, loss: 2.270356\n",
      "Epoch 35, loss: 2.275901\n",
      "Epoch 36, loss: 2.266546\n",
      "Epoch 37, loss: 2.280699\n",
      "Epoch 38, loss: 2.273933\n",
      "Epoch 39, loss: 2.270175\n",
      "Epoch 40, loss: 2.266119\n",
      "Epoch 41, loss: 2.267603\n",
      "Epoch 42, loss: 2.266981\n",
      "Epoch 43, loss: 2.276082\n",
      "Epoch 44, loss: 2.264551\n",
      "Epoch 45, loss: 2.263212\n",
      "Epoch 46, loss: 2.256343\n",
      "Epoch 47, loss: 2.265754\n",
      "Epoch 48, loss: 2.273327\n",
      "Epoch 49, loss: 2.271195\n",
      "Epoch 50, loss: 2.272021\n",
      "Epoch 51, loss: 2.268014\n",
      "Epoch 52, loss: 2.260603\n",
      "Epoch 53, loss: 2.258334\n",
      "Epoch 54, loss: 2.249324\n",
      "Epoch 55, loss: 2.274822\n",
      "Epoch 56, loss: 2.257045\n",
      "Epoch 57, loss: 2.254233\n",
      "Epoch 58, loss: 2.267611\n",
      "Epoch 59, loss: 2.256048\n",
      "Epoch 60, loss: 2.259698\n",
      "Epoch 61, loss: 2.259733\n",
      "Epoch 62, loss: 2.264856\n",
      "Epoch 63, loss: 2.242543\n",
      "Epoch 64, loss: 2.259687\n",
      "Epoch 65, loss: 2.258583\n",
      "Epoch 66, loss: 2.251748\n",
      "Epoch 67, loss: 2.252709\n",
      "Epoch 68, loss: 2.247482\n",
      "Epoch 69, loss: 2.265438\n",
      "Epoch 70, loss: 2.251024\n",
      "Epoch 71, loss: 2.254790\n",
      "Epoch 72, loss: 2.266002\n",
      "Epoch 73, loss: 2.252680\n",
      "Epoch 74, loss: 2.261131\n",
      "Epoch 75, loss: 2.240917\n",
      "Epoch 76, loss: 2.252267\n",
      "Epoch 77, loss: 2.249231\n",
      "Epoch 78, loss: 2.255310\n",
      "Epoch 79, loss: 2.247037\n",
      "Epoch 80, loss: 2.249575\n",
      "Epoch 81, loss: 2.257609\n",
      "Epoch 82, loss: 2.250038\n",
      "Epoch 83, loss: 2.224896\n",
      "Epoch 84, loss: 2.251718\n",
      "Epoch 85, loss: 2.243551\n",
      "Epoch 86, loss: 2.238967\n",
      "Epoch 87, loss: 2.234535\n",
      "Epoch 88, loss: 2.248531\n",
      "Epoch 89, loss: 2.236178\n",
      "Epoch 90, loss: 2.244513\n",
      "Epoch 91, loss: 2.242432\n",
      "Epoch 92, loss: 2.245248\n",
      "Epoch 93, loss: 2.234546\n",
      "Epoch 94, loss: 2.239910\n",
      "Epoch 95, loss: 2.244802\n",
      "Epoch 96, loss: 2.210454\n",
      "Epoch 97, loss: 2.249350\n",
      "Epoch 98, loss: 2.239758\n",
      "Epoch 99, loss: 2.244082\n",
      "Epoch 100, loss: 2.221691\n",
      "Epoch 101, loss: 2.248739\n",
      "Epoch 102, loss: 2.228661\n",
      "Epoch 103, loss: 2.228063\n",
      "Epoch 104, loss: 2.249757\n",
      "Epoch 105, loss: 2.227843\n",
      "Epoch 106, loss: 2.224473\n",
      "Epoch 107, loss: 2.231831\n",
      "Epoch 108, loss: 2.240563\n",
      "Epoch 109, loss: 2.226900\n",
      "Epoch 110, loss: 2.226218\n",
      "Epoch 111, loss: 2.244918\n",
      "Epoch 112, loss: 2.218916\n",
      "Epoch 113, loss: 2.234159\n",
      "Epoch 114, loss: 2.236765\n",
      "Epoch 115, loss: 2.242336\n",
      "Epoch 116, loss: 2.252394\n",
      "Epoch 117, loss: 2.205453\n",
      "Epoch 118, loss: 2.228494\n",
      "Epoch 119, loss: 2.240253\n",
      "Epoch 120, loss: 2.254768\n",
      "Epoch 121, loss: 2.217585\n",
      "Epoch 122, loss: 2.234765\n",
      "Epoch 123, loss: 2.225219\n",
      "Epoch 124, loss: 2.226004\n",
      "Epoch 125, loss: 2.221061\n",
      "Epoch 126, loss: 2.217852\n",
      "Epoch 127, loss: 2.207327\n",
      "Epoch 128, loss: 2.223436\n",
      "Epoch 129, loss: 2.209185\n",
      "Epoch 130, loss: 2.205913\n",
      "Epoch 131, loss: 2.220293\n",
      "Epoch 132, loss: 2.220553\n",
      "Epoch 133, loss: 2.230407\n",
      "Epoch 134, loss: 2.222130\n",
      "Epoch 135, loss: 2.221370\n",
      "Epoch 136, loss: 2.203137\n",
      "Epoch 137, loss: 2.203642\n",
      "Epoch 138, loss: 2.219371\n",
      "Epoch 139, loss: 2.221810\n",
      "Epoch 140, loss: 2.218164\n",
      "Epoch 141, loss: 2.206595\n",
      "Epoch 142, loss: 2.222649\n",
      "Epoch 143, loss: 2.238167\n",
      "Epoch 144, loss: 2.213332\n",
      "Epoch 145, loss: 2.223988\n",
      "Epoch 146, loss: 2.223068\n",
      "Epoch 147, loss: 2.224065\n",
      "Epoch 148, loss: 2.221519\n",
      "Epoch 149, loss: 2.200332\n",
      "Epoch 150, loss: 2.231181\n",
      "Epoch 151, loss: 2.230520\n",
      "Epoch 152, loss: 2.223845\n",
      "Epoch 153, loss: 2.223682\n",
      "Epoch 154, loss: 2.221008\n",
      "Epoch 155, loss: 2.224618\n",
      "Epoch 156, loss: 2.229913\n",
      "Epoch 157, loss: 2.228165\n",
      "Epoch 158, loss: 2.218339\n",
      "Epoch 159, loss: 2.206259\n",
      "Epoch 160, loss: 2.204489\n",
      "Epoch 161, loss: 2.200599\n",
      "Epoch 162, loss: 2.201935\n",
      "Epoch 163, loss: 2.198468\n",
      "Epoch 164, loss: 2.235315\n",
      "Epoch 165, loss: 2.204200\n",
      "Epoch 166, loss: 2.199713\n",
      "Epoch 167, loss: 2.202555\n",
      "Epoch 168, loss: 2.211138\n",
      "Epoch 169, loss: 2.199797\n",
      "Epoch 170, loss: 2.185770\n",
      "Epoch 171, loss: 2.205293\n",
      "Epoch 172, loss: 2.198532\n",
      "Epoch 173, loss: 2.229105\n",
      "Epoch 174, loss: 2.199349\n",
      "Epoch 175, loss: 2.218395\n",
      "Epoch 176, loss: 2.200593\n",
      "Epoch 177, loss: 2.225071\n",
      "Epoch 178, loss: 2.206651\n",
      "Epoch 179, loss: 2.199700\n",
      "Epoch 180, loss: 2.219222\n",
      "Epoch 181, loss: 2.224384\n",
      "Epoch 182, loss: 2.222611\n",
      "Epoch 183, loss: 2.199459\n",
      "Epoch 184, loss: 2.223747\n",
      "Epoch 185, loss: 2.203679\n",
      "Epoch 186, loss: 2.208729\n",
      "Epoch 187, loss: 2.224303\n",
      "Epoch 188, loss: 2.193616\n",
      "Epoch 189, loss: 2.191248\n",
      "Epoch 190, loss: 2.199769\n",
      "Epoch 191, loss: 2.228104\n",
      "Epoch 192, loss: 2.221316\n",
      "Epoch 193, loss: 2.175200\n",
      "Epoch 194, loss: 2.191057\n",
      "Epoch 195, loss: 2.216325\n",
      "Epoch 196, loss: 2.201197\n",
      "Epoch 197, loss: 2.202789\n",
      "Epoch 198, loss: 2.214714\n",
      "Epoch 199, loss: 2.177576\n",
      "Epoch 0, loss: 2.301735\n",
      "Epoch 1, loss: 2.300631\n",
      "Epoch 2, loss: 2.300800\n",
      "Epoch 3, loss: 2.298724\n",
      "Epoch 4, loss: 2.296473\n",
      "Epoch 5, loss: 2.295688\n",
      "Epoch 6, loss: 2.291718\n",
      "Epoch 7, loss: 2.291420\n",
      "Epoch 8, loss: 2.296152\n",
      "Epoch 9, loss: 2.292431\n",
      "Epoch 10, loss: 2.290487\n",
      "Epoch 11, loss: 2.292988\n",
      "Epoch 12, loss: 2.293110\n",
      "Epoch 13, loss: 2.295674\n",
      "Epoch 14, loss: 2.284950\n",
      "Epoch 15, loss: 2.292662\n",
      "Epoch 16, loss: 2.285459\n",
      "Epoch 17, loss: 2.288208\n",
      "Epoch 18, loss: 2.289303\n",
      "Epoch 19, loss: 2.286568\n",
      "Epoch 20, loss: 2.279520\n",
      "Epoch 21, loss: 2.286209\n",
      "Epoch 22, loss: 2.283697\n",
      "Epoch 23, loss: 2.283699\n",
      "Epoch 24, loss: 2.287409\n",
      "Epoch 25, loss: 2.282701\n",
      "Epoch 26, loss: 2.280520\n",
      "Epoch 27, loss: 2.271675\n",
      "Epoch 28, loss: 2.281183\n",
      "Epoch 29, loss: 2.273227\n",
      "Epoch 30, loss: 2.267387\n",
      "Epoch 31, loss: 2.278824\n",
      "Epoch 32, loss: 2.283330\n",
      "Epoch 33, loss: 2.267435\n",
      "Epoch 34, loss: 2.270530\n",
      "Epoch 35, loss: 2.271754\n",
      "Epoch 36, loss: 2.261433\n",
      "Epoch 37, loss: 2.275842\n",
      "Epoch 38, loss: 2.264085\n",
      "Epoch 39, loss: 2.272768\n",
      "Epoch 40, loss: 2.260092\n",
      "Epoch 41, loss: 2.276123\n",
      "Epoch 42, loss: 2.272774\n",
      "Epoch 43, loss: 2.262581\n",
      "Epoch 44, loss: 2.272295\n",
      "Epoch 45, loss: 2.270626\n",
      "Epoch 46, loss: 2.269500\n",
      "Epoch 47, loss: 2.272172\n",
      "Epoch 48, loss: 2.260123\n",
      "Epoch 49, loss: 2.269038\n",
      "Epoch 50, loss: 2.250911\n",
      "Epoch 51, loss: 2.257419\n",
      "Epoch 52, loss: 2.253854\n",
      "Epoch 53, loss: 2.271108\n",
      "Epoch 54, loss: 2.249012\n",
      "Epoch 55, loss: 2.257046\n",
      "Epoch 56, loss: 2.271037\n",
      "Epoch 57, loss: 2.254823\n",
      "Epoch 58, loss: 2.264234\n",
      "Epoch 59, loss: 2.256530\n",
      "Epoch 60, loss: 2.265178\n",
      "Epoch 61, loss: 2.249248\n",
      "Epoch 62, loss: 2.251332\n",
      "Epoch 63, loss: 2.263879\n",
      "Epoch 64, loss: 2.251961\n",
      "Epoch 65, loss: 2.267829\n",
      "Epoch 66, loss: 2.250101\n",
      "Epoch 67, loss: 2.259753\n",
      "Epoch 68, loss: 2.268357\n",
      "Epoch 69, loss: 2.246049\n",
      "Epoch 70, loss: 2.254459\n",
      "Epoch 71, loss: 2.253825\n",
      "Epoch 72, loss: 2.234906\n",
      "Epoch 73, loss: 2.253465\n",
      "Epoch 74, loss: 2.260838\n",
      "Epoch 75, loss: 2.246476\n",
      "Epoch 76, loss: 2.257912\n",
      "Epoch 77, loss: 2.243323\n",
      "Epoch 78, loss: 2.231006\n",
      "Epoch 79, loss: 2.248677\n",
      "Epoch 80, loss: 2.246533\n",
      "Epoch 81, loss: 2.247616\n",
      "Epoch 82, loss: 2.247604\n",
      "Epoch 83, loss: 2.241386\n",
      "Epoch 84, loss: 2.257233\n",
      "Epoch 85, loss: 2.237890\n",
      "Epoch 86, loss: 2.245533\n",
      "Epoch 87, loss: 2.252812\n",
      "Epoch 88, loss: 2.245231\n",
      "Epoch 89, loss: 2.248712\n",
      "Epoch 90, loss: 2.225527\n",
      "Epoch 91, loss: 2.246837\n",
      "Epoch 92, loss: 2.225823\n",
      "Epoch 93, loss: 2.250501\n",
      "Epoch 94, loss: 2.241522\n",
      "Epoch 95, loss: 2.241488\n",
      "Epoch 96, loss: 2.235932\n",
      "Epoch 97, loss: 2.230116\n",
      "Epoch 98, loss: 2.237242\n",
      "Epoch 99, loss: 2.259864\n",
      "Epoch 100, loss: 2.251262\n",
      "Epoch 101, loss: 2.244797\n",
      "Epoch 102, loss: 2.237132\n",
      "Epoch 103, loss: 2.241769\n",
      "Epoch 104, loss: 2.256696\n",
      "Epoch 105, loss: 2.235411\n",
      "Epoch 106, loss: 2.211931\n",
      "Epoch 107, loss: 2.219092\n",
      "Epoch 108, loss: 2.238311\n",
      "Epoch 109, loss: 2.228967\n",
      "Epoch 110, loss: 2.237705\n",
      "Epoch 111, loss: 2.230655\n",
      "Epoch 112, loss: 2.248091\n",
      "Epoch 113, loss: 2.233591\n",
      "Epoch 114, loss: 2.238257\n",
      "Epoch 115, loss: 2.238884\n",
      "Epoch 116, loss: 2.218576\n",
      "Epoch 117, loss: 2.235894\n",
      "Epoch 118, loss: 2.239583\n",
      "Epoch 119, loss: 2.254862\n",
      "Epoch 120, loss: 2.231742\n",
      "Epoch 121, loss: 2.231301\n",
      "Epoch 122, loss: 2.233645\n",
      "Epoch 123, loss: 2.234351\n",
      "Epoch 124, loss: 2.213354\n",
      "Epoch 125, loss: 2.239420\n",
      "Epoch 126, loss: 2.207396\n",
      "Epoch 127, loss: 2.205455\n",
      "Epoch 128, loss: 2.220146\n",
      "Epoch 129, loss: 2.232380\n",
      "Epoch 130, loss: 2.229632\n",
      "Epoch 131, loss: 2.245666\n",
      "Epoch 132, loss: 2.232168\n",
      "Epoch 133, loss: 2.212070\n",
      "Epoch 134, loss: 2.214035\n",
      "Epoch 135, loss: 2.219591\n",
      "Epoch 136, loss: 2.213689\n",
      "Epoch 137, loss: 2.231066\n",
      "Epoch 138, loss: 2.214535\n",
      "Epoch 139, loss: 2.227186\n",
      "Epoch 140, loss: 2.222834\n",
      "Epoch 141, loss: 2.214677\n",
      "Epoch 142, loss: 2.229833\n",
      "Epoch 143, loss: 2.227874\n",
      "Epoch 144, loss: 2.200180\n",
      "Epoch 145, loss: 2.223438\n",
      "Epoch 146, loss: 2.205595\n",
      "Epoch 147, loss: 2.230089\n",
      "Epoch 148, loss: 2.206876\n",
      "Epoch 149, loss: 2.225306\n",
      "Epoch 150, loss: 2.203973\n",
      "Epoch 151, loss: 2.230894\n",
      "Epoch 152, loss: 2.213655\n",
      "Epoch 153, loss: 2.213585\n",
      "Epoch 154, loss: 2.226699\n",
      "Epoch 155, loss: 2.195337\n",
      "Epoch 156, loss: 2.212665\n",
      "Epoch 157, loss: 2.212575\n",
      "Epoch 158, loss: 2.225728\n",
      "Epoch 159, loss: 2.200590\n",
      "Epoch 160, loss: 2.227460\n",
      "Epoch 161, loss: 2.210871\n",
      "Epoch 162, loss: 2.228208\n",
      "Epoch 163, loss: 2.207817\n",
      "Epoch 164, loss: 2.206944\n",
      "Epoch 165, loss: 2.206767\n",
      "Epoch 166, loss: 2.193389\n",
      "Epoch 167, loss: 2.206895\n",
      "Epoch 168, loss: 2.234716\n",
      "Epoch 169, loss: 2.198802\n",
      "Epoch 170, loss: 2.194543\n",
      "Epoch 171, loss: 2.208498\n",
      "Epoch 172, loss: 2.222874\n",
      "Epoch 173, loss: 2.203206\n",
      "Epoch 174, loss: 2.213898\n",
      "Epoch 175, loss: 2.222994\n",
      "Epoch 176, loss: 2.201701\n",
      "Epoch 177, loss: 2.220233\n",
      "Epoch 178, loss: 2.220941\n",
      "Epoch 179, loss: 2.197661\n",
      "Epoch 180, loss: 2.209420\n",
      "Epoch 181, loss: 2.199146\n",
      "Epoch 182, loss: 2.214395\n",
      "Epoch 183, loss: 2.224947\n",
      "Epoch 184, loss: 2.219461\n",
      "Epoch 185, loss: 2.227085\n",
      "Epoch 186, loss: 2.211137\n",
      "Epoch 187, loss: 2.193881\n",
      "Epoch 188, loss: 2.206112\n",
      "Epoch 189, loss: 2.183144\n",
      "Epoch 190, loss: 2.196448\n",
      "Epoch 191, loss: 2.186080\n",
      "Epoch 192, loss: 2.192052\n",
      "Epoch 193, loss: 2.203634\n",
      "Epoch 194, loss: 2.201390\n",
      "Epoch 195, loss: 2.214965\n",
      "Epoch 196, loss: 2.191912\n",
      "Epoch 197, loss: 2.217274\n",
      "Epoch 198, loss: 2.184457\n",
      "Epoch 199, loss: 2.189368\n",
      "Epoch 0, loss: 2.302129\n",
      "Epoch 1, loss: 2.300423\n",
      "Epoch 2, loss: 2.297667\n",
      "Epoch 3, loss: 2.298296\n",
      "Epoch 4, loss: 2.297824\n",
      "Epoch 5, loss: 2.296578\n",
      "Epoch 6, loss: 2.293818\n",
      "Epoch 7, loss: 2.294969\n",
      "Epoch 8, loss: 2.294032\n",
      "Epoch 9, loss: 2.292382\n",
      "Epoch 10, loss: 2.287597\n",
      "Epoch 11, loss: 2.294209\n",
      "Epoch 12, loss: 2.293366\n",
      "Epoch 13, loss: 2.291295\n",
      "Epoch 14, loss: 2.286367\n",
      "Epoch 15, loss: 2.291076\n",
      "Epoch 16, loss: 2.285752\n",
      "Epoch 17, loss: 2.290092\n",
      "Epoch 18, loss: 2.286509\n",
      "Epoch 19, loss: 2.282915\n",
      "Epoch 20, loss: 2.287176\n",
      "Epoch 21, loss: 2.287994\n",
      "Epoch 22, loss: 2.279103\n",
      "Epoch 23, loss: 2.283140\n",
      "Epoch 24, loss: 2.280689\n",
      "Epoch 25, loss: 2.282042\n",
      "Epoch 26, loss: 2.281763\n",
      "Epoch 27, loss: 2.278217\n",
      "Epoch 28, loss: 2.284176\n",
      "Epoch 29, loss: 2.275853\n",
      "Epoch 30, loss: 2.280278\n",
      "Epoch 31, loss: 2.274019\n",
      "Epoch 32, loss: 2.272897\n",
      "Epoch 33, loss: 2.274112\n",
      "Epoch 34, loss: 2.282380\n",
      "Epoch 35, loss: 2.270541\n",
      "Epoch 36, loss: 2.277493\n",
      "Epoch 37, loss: 2.265591\n",
      "Epoch 38, loss: 2.273610\n",
      "Epoch 39, loss: 2.263384\n",
      "Epoch 40, loss: 2.263825\n",
      "Epoch 41, loss: 2.273294\n",
      "Epoch 42, loss: 2.265441\n",
      "Epoch 43, loss: 2.271572\n",
      "Epoch 44, loss: 2.267402\n",
      "Epoch 45, loss: 2.276080\n",
      "Epoch 46, loss: 2.263682\n",
      "Epoch 47, loss: 2.264505\n",
      "Epoch 48, loss: 2.266758\n",
      "Epoch 49, loss: 2.264160\n",
      "Epoch 50, loss: 2.262614\n",
      "Epoch 51, loss: 2.265243\n",
      "Epoch 52, loss: 2.256829\n",
      "Epoch 53, loss: 2.268528\n",
      "Epoch 54, loss: 2.263165\n",
      "Epoch 55, loss: 2.257292\n",
      "Epoch 56, loss: 2.271366\n",
      "Epoch 57, loss: 2.249133\n",
      "Epoch 58, loss: 2.255806\n",
      "Epoch 59, loss: 2.257141\n",
      "Epoch 60, loss: 2.261399\n",
      "Epoch 61, loss: 2.254205\n",
      "Epoch 62, loss: 2.258710\n",
      "Epoch 63, loss: 2.251830\n",
      "Epoch 64, loss: 2.246886\n",
      "Epoch 65, loss: 2.257862\n",
      "Epoch 66, loss: 2.253254\n",
      "Epoch 67, loss: 2.265297\n",
      "Epoch 68, loss: 2.247196\n",
      "Epoch 69, loss: 2.254780\n",
      "Epoch 70, loss: 2.244639\n",
      "Epoch 71, loss: 2.248510\n",
      "Epoch 72, loss: 2.264739\n",
      "Epoch 73, loss: 2.260717\n",
      "Epoch 74, loss: 2.256524\n",
      "Epoch 75, loss: 2.248260\n",
      "Epoch 76, loss: 2.264449\n",
      "Epoch 77, loss: 2.244954\n",
      "Epoch 78, loss: 2.256513\n",
      "Epoch 79, loss: 2.247579\n",
      "Epoch 80, loss: 2.247926\n",
      "Epoch 81, loss: 2.237210\n",
      "Epoch 82, loss: 2.241995\n",
      "Epoch 83, loss: 2.258771\n",
      "Epoch 84, loss: 2.232269\n",
      "Epoch 85, loss: 2.250791\n",
      "Epoch 86, loss: 2.236052\n",
      "Epoch 87, loss: 2.260371\n",
      "Epoch 88, loss: 2.240492\n",
      "Epoch 89, loss: 2.237552\n",
      "Epoch 90, loss: 2.236867\n",
      "Epoch 91, loss: 2.240260\n",
      "Epoch 92, loss: 2.234258\n",
      "Epoch 93, loss: 2.246003\n",
      "Epoch 94, loss: 2.246058\n",
      "Epoch 95, loss: 2.238976\n",
      "Epoch 96, loss: 2.253234\n",
      "Epoch 97, loss: 2.241119\n",
      "Epoch 98, loss: 2.233402\n",
      "Epoch 99, loss: 2.229989\n",
      "Epoch 100, loss: 2.232675\n",
      "Epoch 101, loss: 2.241485\n",
      "Epoch 102, loss: 2.222915\n",
      "Epoch 103, loss: 2.238419\n",
      "Epoch 104, loss: 2.231413\n",
      "Epoch 105, loss: 2.258204\n",
      "Epoch 106, loss: 2.248248\n",
      "Epoch 107, loss: 2.228920\n",
      "Epoch 108, loss: 2.231651\n",
      "Epoch 109, loss: 2.225883\n",
      "Epoch 110, loss: 2.233264\n",
      "Epoch 111, loss: 2.235821\n",
      "Epoch 112, loss: 2.211092\n",
      "Epoch 113, loss: 2.220765\n",
      "Epoch 114, loss: 2.232346\n",
      "Epoch 115, loss: 2.220712\n",
      "Epoch 116, loss: 2.238493\n",
      "Epoch 117, loss: 2.211011\n",
      "Epoch 118, loss: 2.222705\n",
      "Epoch 119, loss: 2.247158\n",
      "Epoch 120, loss: 2.227730\n",
      "Epoch 121, loss: 2.231869\n",
      "Epoch 122, loss: 2.240492\n",
      "Epoch 123, loss: 2.227846\n",
      "Epoch 124, loss: 2.239288\n",
      "Epoch 125, loss: 2.233642\n",
      "Epoch 126, loss: 2.229731\n",
      "Epoch 127, loss: 2.232483\n",
      "Epoch 128, loss: 2.200338\n",
      "Epoch 129, loss: 2.245741\n",
      "Epoch 130, loss: 2.231848\n",
      "Epoch 131, loss: 2.228731\n",
      "Epoch 132, loss: 2.219027\n",
      "Epoch 133, loss: 2.229266\n",
      "Epoch 134, loss: 2.212374\n",
      "Epoch 135, loss: 2.228607\n",
      "Epoch 136, loss: 2.214200\n",
      "Epoch 137, loss: 2.200201\n",
      "Epoch 138, loss: 2.212400\n",
      "Epoch 139, loss: 2.214711\n",
      "Epoch 140, loss: 2.226099\n",
      "Epoch 141, loss: 2.216612\n",
      "Epoch 142, loss: 2.233578\n",
      "Epoch 143, loss: 2.216056\n",
      "Epoch 144, loss: 2.220381\n",
      "Epoch 145, loss: 2.252339\n",
      "Epoch 146, loss: 2.234932\n",
      "Epoch 147, loss: 2.212430\n",
      "Epoch 148, loss: 2.205642\n",
      "Epoch 149, loss: 2.215859\n",
      "Epoch 150, loss: 2.230767\n",
      "Epoch 151, loss: 2.220905\n",
      "Epoch 152, loss: 2.240320\n",
      "Epoch 153, loss: 2.213532\n",
      "Epoch 154, loss: 2.185191\n",
      "Epoch 155, loss: 2.208406\n",
      "Epoch 156, loss: 2.215571\n",
      "Epoch 157, loss: 2.233666\n",
      "Epoch 158, loss: 2.228945\n",
      "Epoch 159, loss: 2.224019\n",
      "Epoch 160, loss: 2.200211\n",
      "Epoch 161, loss: 2.211523\n",
      "Epoch 162, loss: 2.220928\n",
      "Epoch 163, loss: 2.180413\n",
      "Epoch 164, loss: 2.183887\n",
      "Epoch 165, loss: 2.215811\n",
      "Epoch 166, loss: 2.187771\n",
      "Epoch 167, loss: 2.216310\n",
      "Epoch 168, loss: 2.223218\n",
      "Epoch 169, loss: 2.213807\n",
      "Epoch 170, loss: 2.204597\n",
      "Epoch 171, loss: 2.207799\n",
      "Epoch 172, loss: 2.212272\n",
      "Epoch 173, loss: 2.190164\n",
      "Epoch 174, loss: 2.213684\n",
      "Epoch 175, loss: 2.217286\n",
      "Epoch 176, loss: 2.175573\n",
      "Epoch 177, loss: 2.190128\n",
      "Epoch 178, loss: 2.215714\n",
      "Epoch 179, loss: 2.199233\n",
      "Epoch 180, loss: 2.213456\n",
      "Epoch 181, loss: 2.198477\n",
      "Epoch 182, loss: 2.226415\n",
      "Epoch 183, loss: 2.189646\n",
      "Epoch 184, loss: 2.203420\n",
      "Epoch 185, loss: 2.200878\n",
      "Epoch 186, loss: 2.205884\n",
      "Epoch 187, loss: 2.173906\n",
      "Epoch 188, loss: 2.160994\n",
      "Epoch 189, loss: 2.201256\n",
      "Epoch 190, loss: 2.220697\n",
      "Epoch 191, loss: 2.185583\n",
      "Epoch 192, loss: 2.202771\n",
      "Epoch 193, loss: 2.217678\n",
      "Epoch 194, loss: 2.207147\n",
      "Epoch 195, loss: 2.215324\n",
      "Epoch 196, loss: 2.206569\n",
      "Epoch 197, loss: 2.214488\n",
      "Epoch 198, loss: 2.211831\n",
      "Epoch 199, loss: 2.191940\n",
      "Epoch 0, loss: 2.301768\n",
      "Epoch 1, loss: 2.303202\n",
      "Epoch 2, loss: 2.302646\n",
      "Epoch 3, loss: 2.302404\n",
      "Epoch 4, loss: 2.303433\n",
      "Epoch 5, loss: 2.302129\n",
      "Epoch 6, loss: 2.300904\n",
      "Epoch 7, loss: 2.301225\n",
      "Epoch 8, loss: 2.302707\n",
      "Epoch 9, loss: 2.301979\n",
      "Epoch 10, loss: 2.300716\n",
      "Epoch 11, loss: 2.300897\n",
      "Epoch 12, loss: 2.300401\n",
      "Epoch 13, loss: 2.300507\n",
      "Epoch 14, loss: 2.300763\n",
      "Epoch 15, loss: 2.300794\n",
      "Epoch 16, loss: 2.300655\n",
      "Epoch 17, loss: 2.301184\n",
      "Epoch 18, loss: 2.300415\n",
      "Epoch 19, loss: 2.300183\n",
      "Epoch 20, loss: 2.300324\n",
      "Epoch 21, loss: 2.301015\n",
      "Epoch 22, loss: 2.300541\n",
      "Epoch 23, loss: 2.299770\n",
      "Epoch 24, loss: 2.300258\n",
      "Epoch 25, loss: 2.298641\n",
      "Epoch 26, loss: 2.299767\n",
      "Epoch 27, loss: 2.299556\n",
      "Epoch 28, loss: 2.299493\n",
      "Epoch 29, loss: 2.299726\n",
      "Epoch 30, loss: 2.298489\n",
      "Epoch 31, loss: 2.299773\n",
      "Epoch 32, loss: 2.299561\n",
      "Epoch 33, loss: 2.300018\n",
      "Epoch 34, loss: 2.298884\n",
      "Epoch 35, loss: 2.298823\n",
      "Epoch 36, loss: 2.297857\n",
      "Epoch 37, loss: 2.299124\n",
      "Epoch 38, loss: 2.298756\n",
      "Epoch 39, loss: 2.298262\n",
      "Epoch 40, loss: 2.297630\n",
      "Epoch 41, loss: 2.300057\n",
      "Epoch 42, loss: 2.296780\n",
      "Epoch 43, loss: 2.299416\n",
      "Epoch 44, loss: 2.297972\n",
      "Epoch 45, loss: 2.298798\n",
      "Epoch 46, loss: 2.297125\n",
      "Epoch 47, loss: 2.300320\n",
      "Epoch 48, loss: 2.296503\n",
      "Epoch 49, loss: 2.297206\n",
      "Epoch 50, loss: 2.297678\n",
      "Epoch 51, loss: 2.295386\n",
      "Epoch 52, loss: 2.298823\n",
      "Epoch 53, loss: 2.299984\n",
      "Epoch 54, loss: 2.298266\n",
      "Epoch 55, loss: 2.298445\n",
      "Epoch 56, loss: 2.296018\n",
      "Epoch 57, loss: 2.293955\n",
      "Epoch 58, loss: 2.293544\n",
      "Epoch 59, loss: 2.297933\n",
      "Epoch 60, loss: 2.296878\n",
      "Epoch 61, loss: 2.299252\n",
      "Epoch 62, loss: 2.296704\n",
      "Epoch 63, loss: 2.297080\n",
      "Epoch 64, loss: 2.296752\n",
      "Epoch 65, loss: 2.296030\n",
      "Epoch 66, loss: 2.296714\n",
      "Epoch 67, loss: 2.292626\n",
      "Epoch 68, loss: 2.298876\n",
      "Epoch 69, loss: 2.294463\n",
      "Epoch 70, loss: 2.293619\n",
      "Epoch 71, loss: 2.294193\n",
      "Epoch 72, loss: 2.296057\n",
      "Epoch 73, loss: 2.294224\n",
      "Epoch 74, loss: 2.296006\n",
      "Epoch 75, loss: 2.297610\n",
      "Epoch 76, loss: 2.294625\n",
      "Epoch 77, loss: 2.296302\n",
      "Epoch 78, loss: 2.293777\n",
      "Epoch 79, loss: 2.295422\n",
      "Epoch 80, loss: 2.293928\n",
      "Epoch 81, loss: 2.294437\n",
      "Epoch 82, loss: 2.295817\n",
      "Epoch 83, loss: 2.294605\n",
      "Epoch 84, loss: 2.294844\n",
      "Epoch 85, loss: 2.293008\n",
      "Epoch 86, loss: 2.294338\n",
      "Epoch 87, loss: 2.292312\n",
      "Epoch 88, loss: 2.290189\n",
      "Epoch 89, loss: 2.292825\n",
      "Epoch 90, loss: 2.292054\n",
      "Epoch 91, loss: 2.294738\n",
      "Epoch 92, loss: 2.295472\n",
      "Epoch 93, loss: 2.291825\n",
      "Epoch 94, loss: 2.289544\n",
      "Epoch 95, loss: 2.297132\n",
      "Epoch 96, loss: 2.294282\n",
      "Epoch 97, loss: 2.297401\n",
      "Epoch 98, loss: 2.292837\n",
      "Epoch 99, loss: 2.292070\n",
      "Epoch 100, loss: 2.295110\n",
      "Epoch 101, loss: 2.290998\n",
      "Epoch 102, loss: 2.296297\n",
      "Epoch 103, loss: 2.291156\n",
      "Epoch 104, loss: 2.295694\n",
      "Epoch 105, loss: 2.293047\n",
      "Epoch 106, loss: 2.287786\n",
      "Epoch 107, loss: 2.291837\n",
      "Epoch 108, loss: 2.293874\n",
      "Epoch 109, loss: 2.293989\n",
      "Epoch 110, loss: 2.288240\n",
      "Epoch 111, loss: 2.292050\n",
      "Epoch 112, loss: 2.290021\n",
      "Epoch 113, loss: 2.292534\n",
      "Epoch 114, loss: 2.296709\n",
      "Epoch 115, loss: 2.295221\n",
      "Epoch 116, loss: 2.294517\n",
      "Epoch 117, loss: 2.289612\n",
      "Epoch 118, loss: 2.289395\n",
      "Epoch 119, loss: 2.290062\n",
      "Epoch 120, loss: 2.290831\n",
      "Epoch 121, loss: 2.289117\n",
      "Epoch 122, loss: 2.291125\n",
      "Epoch 123, loss: 2.293774\n",
      "Epoch 124, loss: 2.292043\n",
      "Epoch 125, loss: 2.289050\n",
      "Epoch 126, loss: 2.290774\n",
      "Epoch 127, loss: 2.293200\n",
      "Epoch 128, loss: 2.291439\n",
      "Epoch 129, loss: 2.297280\n",
      "Epoch 130, loss: 2.288472\n",
      "Epoch 131, loss: 2.288910\n",
      "Epoch 132, loss: 2.288482\n",
      "Epoch 133, loss: 2.291179\n",
      "Epoch 134, loss: 2.291846\n",
      "Epoch 135, loss: 2.290956\n",
      "Epoch 136, loss: 2.289554\n",
      "Epoch 137, loss: 2.290442\n",
      "Epoch 138, loss: 2.288175\n",
      "Epoch 139, loss: 2.290609\n",
      "Epoch 140, loss: 2.289157\n",
      "Epoch 141, loss: 2.286643\n",
      "Epoch 142, loss: 2.284647\n",
      "Epoch 143, loss: 2.288590\n",
      "Epoch 144, loss: 2.294317\n",
      "Epoch 145, loss: 2.292301\n",
      "Epoch 146, loss: 2.292526\n",
      "Epoch 147, loss: 2.287678\n",
      "Epoch 148, loss: 2.288296\n",
      "Epoch 149, loss: 2.287669\n",
      "Epoch 150, loss: 2.289377\n",
      "Epoch 151, loss: 2.289563\n",
      "Epoch 152, loss: 2.286098\n",
      "Epoch 153, loss: 2.285886\n",
      "Epoch 154, loss: 2.286502\n",
      "Epoch 155, loss: 2.289655\n",
      "Epoch 156, loss: 2.289064\n",
      "Epoch 157, loss: 2.286336\n",
      "Epoch 158, loss: 2.288684\n",
      "Epoch 159, loss: 2.289575\n",
      "Epoch 160, loss: 2.284893\n",
      "Epoch 161, loss: 2.290008\n",
      "Epoch 162, loss: 2.287396\n",
      "Epoch 163, loss: 2.289044\n",
      "Epoch 164, loss: 2.286842\n",
      "Epoch 165, loss: 2.282991\n",
      "Epoch 166, loss: 2.295489\n",
      "Epoch 167, loss: 2.291475\n",
      "Epoch 168, loss: 2.289270\n",
      "Epoch 169, loss: 2.288535\n",
      "Epoch 170, loss: 2.288244\n",
      "Epoch 171, loss: 2.284457\n",
      "Epoch 172, loss: 2.284351\n",
      "Epoch 173, loss: 2.287678\n",
      "Epoch 174, loss: 2.291125\n",
      "Epoch 175, loss: 2.283330\n",
      "Epoch 176, loss: 2.289206\n",
      "Epoch 177, loss: 2.285117\n",
      "Epoch 178, loss: 2.282336\n",
      "Epoch 179, loss: 2.284365\n",
      "Epoch 180, loss: 2.283475\n",
      "Epoch 181, loss: 2.285052\n",
      "Epoch 182, loss: 2.282081\n",
      "Epoch 183, loss: 2.292440\n",
      "Epoch 184, loss: 2.291659\n",
      "Epoch 185, loss: 2.288653\n",
      "Epoch 186, loss: 2.287853\n",
      "Epoch 187, loss: 2.292818\n",
      "Epoch 188, loss: 2.288532\n",
      "Epoch 189, loss: 2.284549\n",
      "Epoch 190, loss: 2.281103\n",
      "Epoch 191, loss: 2.285795\n",
      "Epoch 192, loss: 2.285805\n",
      "Epoch 193, loss: 2.287722\n",
      "Epoch 194, loss: 2.290665\n",
      "Epoch 195, loss: 2.287511\n",
      "Epoch 196, loss: 2.287010\n",
      "Epoch 197, loss: 2.284331\n",
      "Epoch 198, loss: 2.286515\n",
      "Epoch 199, loss: 2.283061\n",
      "Epoch 0, loss: 2.301771\n",
      "Epoch 1, loss: 2.301801\n",
      "Epoch 2, loss: 2.302836\n",
      "Epoch 3, loss: 2.302236\n",
      "Epoch 4, loss: 2.302114\n",
      "Epoch 5, loss: 2.301729\n",
      "Epoch 6, loss: 2.301990\n",
      "Epoch 7, loss: 2.301763\n",
      "Epoch 8, loss: 2.302154\n",
      "Epoch 9, loss: 2.302165\n",
      "Epoch 10, loss: 2.301884\n",
      "Epoch 11, loss: 2.301087\n",
      "Epoch 12, loss: 2.301613\n",
      "Epoch 13, loss: 2.301587\n",
      "Epoch 14, loss: 2.300639\n",
      "Epoch 15, loss: 2.301049\n",
      "Epoch 16, loss: 2.301050\n",
      "Epoch 17, loss: 2.300079\n",
      "Epoch 18, loss: 2.299898\n",
      "Epoch 19, loss: 2.298835\n",
      "Epoch 20, loss: 2.300349\n",
      "Epoch 21, loss: 2.300783\n",
      "Epoch 22, loss: 2.300154\n",
      "Epoch 23, loss: 2.300723\n",
      "Epoch 24, loss: 2.300297\n",
      "Epoch 25, loss: 2.299757\n",
      "Epoch 26, loss: 2.300230\n",
      "Epoch 27, loss: 2.299682\n",
      "Epoch 28, loss: 2.300431\n",
      "Epoch 29, loss: 2.299867\n",
      "Epoch 30, loss: 2.298177\n",
      "Epoch 31, loss: 2.298986\n",
      "Epoch 32, loss: 2.299464\n",
      "Epoch 33, loss: 2.298929\n",
      "Epoch 34, loss: 2.299550\n",
      "Epoch 35, loss: 2.300027\n",
      "Epoch 36, loss: 2.299648\n",
      "Epoch 37, loss: 2.298236\n",
      "Epoch 38, loss: 2.300592\n",
      "Epoch 39, loss: 2.299407\n",
      "Epoch 40, loss: 2.296185\n",
      "Epoch 41, loss: 2.298028\n",
      "Epoch 42, loss: 2.298340\n",
      "Epoch 43, loss: 2.296797\n",
      "Epoch 44, loss: 2.299682\n",
      "Epoch 45, loss: 2.300388\n",
      "Epoch 46, loss: 2.298078\n",
      "Epoch 47, loss: 2.296636\n",
      "Epoch 48, loss: 2.298099\n",
      "Epoch 49, loss: 2.298134\n",
      "Epoch 50, loss: 2.298220\n",
      "Epoch 51, loss: 2.299224\n",
      "Epoch 52, loss: 2.299116\n",
      "Epoch 53, loss: 2.294596\n",
      "Epoch 54, loss: 2.296571\n",
      "Epoch 55, loss: 2.296175\n",
      "Epoch 56, loss: 2.299358\n",
      "Epoch 57, loss: 2.295367\n",
      "Epoch 58, loss: 2.295575\n",
      "Epoch 59, loss: 2.297269\n",
      "Epoch 60, loss: 2.297304\n",
      "Epoch 61, loss: 2.294492\n",
      "Epoch 62, loss: 2.296729\n",
      "Epoch 63, loss: 2.297252\n",
      "Epoch 64, loss: 2.297217\n",
      "Epoch 65, loss: 2.290753\n",
      "Epoch 66, loss: 2.297430\n",
      "Epoch 67, loss: 2.294612\n",
      "Epoch 68, loss: 2.296475\n",
      "Epoch 69, loss: 2.295757\n",
      "Epoch 70, loss: 2.299547\n",
      "Epoch 71, loss: 2.294859\n",
      "Epoch 72, loss: 2.295719\n",
      "Epoch 73, loss: 2.294646\n",
      "Epoch 74, loss: 2.294914\n",
      "Epoch 75, loss: 2.299166\n",
      "Epoch 76, loss: 2.297799\n",
      "Epoch 77, loss: 2.295884\n",
      "Epoch 78, loss: 2.294490\n",
      "Epoch 79, loss: 2.292773\n",
      "Epoch 80, loss: 2.294374\n",
      "Epoch 81, loss: 2.293687\n",
      "Epoch 82, loss: 2.293659\n",
      "Epoch 83, loss: 2.293915\n",
      "Epoch 84, loss: 2.296139\n",
      "Epoch 85, loss: 2.296986\n",
      "Epoch 86, loss: 2.297947\n",
      "Epoch 87, loss: 2.292514\n",
      "Epoch 88, loss: 2.294483\n",
      "Epoch 89, loss: 2.293741\n",
      "Epoch 90, loss: 2.295567\n",
      "Epoch 91, loss: 2.292811\n",
      "Epoch 92, loss: 2.293770\n",
      "Epoch 93, loss: 2.293435\n",
      "Epoch 94, loss: 2.294769\n",
      "Epoch 95, loss: 2.297693\n",
      "Epoch 96, loss: 2.288178\n",
      "Epoch 97, loss: 2.289431\n",
      "Epoch 98, loss: 2.293144\n",
      "Epoch 99, loss: 2.293767\n",
      "Epoch 100, loss: 2.295893\n",
      "Epoch 101, loss: 2.290133\n",
      "Epoch 102, loss: 2.293172\n",
      "Epoch 103, loss: 2.294737\n",
      "Epoch 104, loss: 2.293102\n",
      "Epoch 105, loss: 2.298010\n",
      "Epoch 106, loss: 2.289758\n",
      "Epoch 107, loss: 2.292678\n",
      "Epoch 108, loss: 2.292527\n",
      "Epoch 109, loss: 2.293150\n",
      "Epoch 110, loss: 2.292302\n",
      "Epoch 111, loss: 2.292713\n",
      "Epoch 112, loss: 2.288106\n",
      "Epoch 113, loss: 2.295037\n",
      "Epoch 114, loss: 2.290170\n",
      "Epoch 115, loss: 2.292152\n",
      "Epoch 116, loss: 2.289716\n",
      "Epoch 117, loss: 2.290346\n",
      "Epoch 118, loss: 2.296326\n",
      "Epoch 119, loss: 2.291147\n",
      "Epoch 120, loss: 2.290247\n",
      "Epoch 121, loss: 2.290155\n",
      "Epoch 122, loss: 2.290092\n",
      "Epoch 123, loss: 2.294393\n",
      "Epoch 124, loss: 2.293103\n",
      "Epoch 125, loss: 2.288773\n",
      "Epoch 126, loss: 2.288387\n",
      "Epoch 127, loss: 2.291905\n",
      "Epoch 128, loss: 2.286013\n",
      "Epoch 129, loss: 2.292267\n",
      "Epoch 130, loss: 2.291583\n",
      "Epoch 131, loss: 2.289003\n",
      "Epoch 132, loss: 2.291311\n",
      "Epoch 133, loss: 2.293874\n",
      "Epoch 134, loss: 2.288034\n",
      "Epoch 135, loss: 2.287981\n",
      "Epoch 136, loss: 2.290460\n",
      "Epoch 137, loss: 2.290029\n",
      "Epoch 138, loss: 2.288703\n",
      "Epoch 139, loss: 2.285689\n",
      "Epoch 140, loss: 2.292271\n",
      "Epoch 141, loss: 2.289243\n",
      "Epoch 142, loss: 2.290327\n",
      "Epoch 143, loss: 2.291907\n",
      "Epoch 144, loss: 2.294281\n",
      "Epoch 145, loss: 2.290237\n",
      "Epoch 146, loss: 2.292154\n",
      "Epoch 147, loss: 2.288610\n",
      "Epoch 148, loss: 2.290894\n",
      "Epoch 149, loss: 2.291323\n",
      "Epoch 150, loss: 2.292885\n",
      "Epoch 151, loss: 2.288806\n",
      "Epoch 152, loss: 2.292627\n",
      "Epoch 153, loss: 2.285790\n",
      "Epoch 154, loss: 2.288536\n",
      "Epoch 155, loss: 2.284126\n",
      "Epoch 156, loss: 2.289910\n",
      "Epoch 157, loss: 2.288666\n",
      "Epoch 158, loss: 2.283433\n",
      "Epoch 159, loss: 2.288280\n",
      "Epoch 160, loss: 2.284458\n",
      "Epoch 161, loss: 2.288648\n",
      "Epoch 162, loss: 2.287604\n",
      "Epoch 163, loss: 2.289654\n",
      "Epoch 164, loss: 2.283753\n",
      "Epoch 165, loss: 2.287051\n",
      "Epoch 166, loss: 2.286715\n",
      "Epoch 167, loss: 2.291569\n",
      "Epoch 168, loss: 2.289339\n",
      "Epoch 169, loss: 2.287323\n",
      "Epoch 170, loss: 2.285975\n",
      "Epoch 171, loss: 2.282980\n",
      "Epoch 172, loss: 2.291652\n",
      "Epoch 173, loss: 2.279160\n",
      "Epoch 174, loss: 2.287349\n",
      "Epoch 175, loss: 2.286837\n",
      "Epoch 176, loss: 2.289794\n",
      "Epoch 177, loss: 2.284022\n",
      "Epoch 178, loss: 2.287745\n",
      "Epoch 179, loss: 2.282112\n",
      "Epoch 180, loss: 2.287697\n",
      "Epoch 181, loss: 2.285027\n",
      "Epoch 182, loss: 2.284646\n",
      "Epoch 183, loss: 2.287859\n",
      "Epoch 184, loss: 2.284053\n",
      "Epoch 185, loss: 2.286864\n",
      "Epoch 186, loss: 2.286501\n",
      "Epoch 187, loss: 2.285809\n",
      "Epoch 188, loss: 2.291108\n",
      "Epoch 189, loss: 2.281090\n",
      "Epoch 190, loss: 2.281911\n",
      "Epoch 191, loss: 2.285369\n",
      "Epoch 192, loss: 2.282304\n",
      "Epoch 193, loss: 2.284124\n",
      "Epoch 194, loss: 2.281656\n",
      "Epoch 195, loss: 2.286627\n",
      "Epoch 196, loss: 2.291401\n",
      "Epoch 197, loss: 2.285764\n",
      "Epoch 198, loss: 2.289450\n",
      "Epoch 199, loss: 2.289842\n",
      "Epoch 0, loss: 2.302848\n",
      "Epoch 1, loss: 2.302363\n",
      "Epoch 2, loss: 2.302331\n",
      "Epoch 3, loss: 2.301634\n",
      "Epoch 4, loss: 2.301261\n",
      "Epoch 5, loss: 2.302085\n",
      "Epoch 6, loss: 2.302478\n",
      "Epoch 7, loss: 2.300776\n",
      "Epoch 8, loss: 2.300690\n",
      "Epoch 9, loss: 2.301700\n",
      "Epoch 10, loss: 2.302105\n",
      "Epoch 11, loss: 2.301545\n",
      "Epoch 12, loss: 2.301523\n",
      "Epoch 13, loss: 2.301330\n",
      "Epoch 14, loss: 2.302018\n",
      "Epoch 15, loss: 2.300394\n",
      "Epoch 16, loss: 2.300759\n",
      "Epoch 17, loss: 2.300536\n",
      "Epoch 18, loss: 2.301423\n",
      "Epoch 19, loss: 2.299925\n",
      "Epoch 20, loss: 2.299701\n",
      "Epoch 21, loss: 2.299105\n",
      "Epoch 22, loss: 2.301018\n",
      "Epoch 23, loss: 2.300740\n",
      "Epoch 24, loss: 2.299256\n",
      "Epoch 25, loss: 2.299952\n",
      "Epoch 26, loss: 2.299323\n",
      "Epoch 27, loss: 2.299424\n",
      "Epoch 28, loss: 2.300726\n",
      "Epoch 29, loss: 2.298864\n",
      "Epoch 30, loss: 2.300104\n",
      "Epoch 31, loss: 2.299787\n",
      "Epoch 32, loss: 2.299663\n",
      "Epoch 33, loss: 2.298825\n",
      "Epoch 34, loss: 2.299859\n",
      "Epoch 35, loss: 2.298456\n",
      "Epoch 36, loss: 2.301551\n",
      "Epoch 37, loss: 2.299270\n",
      "Epoch 38, loss: 2.298937\n",
      "Epoch 39, loss: 2.298990\n",
      "Epoch 40, loss: 2.298464\n",
      "Epoch 41, loss: 2.298814\n",
      "Epoch 42, loss: 2.298182\n",
      "Epoch 43, loss: 2.297778\n",
      "Epoch 44, loss: 2.298529\n",
      "Epoch 45, loss: 2.296738\n",
      "Epoch 46, loss: 2.300233\n",
      "Epoch 47, loss: 2.297989\n",
      "Epoch 48, loss: 2.299970\n",
      "Epoch 49, loss: 2.297450\n",
      "Epoch 50, loss: 2.297879\n",
      "Epoch 51, loss: 2.297603\n",
      "Epoch 52, loss: 2.298263\n",
      "Epoch 53, loss: 2.295780\n",
      "Epoch 54, loss: 2.296805\n",
      "Epoch 55, loss: 2.298900\n",
      "Epoch 56, loss: 2.294762\n",
      "Epoch 57, loss: 2.296570\n",
      "Epoch 58, loss: 2.298112\n",
      "Epoch 59, loss: 2.297004\n",
      "Epoch 60, loss: 2.295216\n",
      "Epoch 61, loss: 2.294887\n",
      "Epoch 62, loss: 2.295532\n",
      "Epoch 63, loss: 2.297541\n",
      "Epoch 64, loss: 2.293583\n",
      "Epoch 65, loss: 2.298998\n",
      "Epoch 66, loss: 2.295859\n",
      "Epoch 67, loss: 2.298288\n",
      "Epoch 68, loss: 2.292933\n",
      "Epoch 69, loss: 2.296862\n",
      "Epoch 70, loss: 2.294575\n",
      "Epoch 71, loss: 2.296397\n",
      "Epoch 72, loss: 2.294892\n",
      "Epoch 73, loss: 2.298685\n",
      "Epoch 74, loss: 2.294567\n",
      "Epoch 75, loss: 2.296488\n",
      "Epoch 76, loss: 2.295849\n",
      "Epoch 77, loss: 2.294594\n",
      "Epoch 78, loss: 2.294124\n",
      "Epoch 79, loss: 2.292552\n",
      "Epoch 80, loss: 2.293675\n",
      "Epoch 81, loss: 2.296621\n",
      "Epoch 82, loss: 2.295915\n",
      "Epoch 83, loss: 2.295305\n",
      "Epoch 84, loss: 2.293586\n",
      "Epoch 85, loss: 2.295619\n",
      "Epoch 86, loss: 2.291841\n",
      "Epoch 87, loss: 2.292239\n",
      "Epoch 88, loss: 2.290684\n",
      "Epoch 89, loss: 2.293103\n",
      "Epoch 90, loss: 2.296765\n",
      "Epoch 91, loss: 2.294367\n",
      "Epoch 92, loss: 2.296287\n",
      "Epoch 93, loss: 2.294887\n",
      "Epoch 94, loss: 2.291502\n",
      "Epoch 95, loss: 2.294612\n",
      "Epoch 96, loss: 2.294703\n",
      "Epoch 97, loss: 2.298774\n",
      "Epoch 98, loss: 2.296022\n",
      "Epoch 99, loss: 2.291963\n",
      "Epoch 100, loss: 2.293770\n",
      "Epoch 101, loss: 2.294066\n",
      "Epoch 102, loss: 2.291205\n",
      "Epoch 103, loss: 2.292831\n",
      "Epoch 104, loss: 2.295498\n",
      "Epoch 105, loss: 2.290582\n",
      "Epoch 106, loss: 2.295828\n",
      "Epoch 107, loss: 2.296568\n",
      "Epoch 108, loss: 2.292349\n",
      "Epoch 109, loss: 2.290034\n",
      "Epoch 110, loss: 2.288103\n",
      "Epoch 111, loss: 2.291917\n",
      "Epoch 112, loss: 2.295293\n",
      "Epoch 113, loss: 2.293358\n",
      "Epoch 114, loss: 2.293525\n",
      "Epoch 115, loss: 2.291149\n",
      "Epoch 116, loss: 2.291769\n",
      "Epoch 117, loss: 2.293633\n",
      "Epoch 118, loss: 2.294477\n",
      "Epoch 119, loss: 2.291958\n",
      "Epoch 120, loss: 2.293342\n",
      "Epoch 121, loss: 2.292307\n",
      "Epoch 122, loss: 2.288107\n",
      "Epoch 123, loss: 2.291391\n",
      "Epoch 124, loss: 2.290675\n",
      "Epoch 125, loss: 2.297625\n",
      "Epoch 126, loss: 2.284975\n",
      "Epoch 127, loss: 2.292975\n",
      "Epoch 128, loss: 2.295628\n",
      "Epoch 129, loss: 2.288985\n",
      "Epoch 130, loss: 2.286358\n",
      "Epoch 131, loss: 2.290820\n",
      "Epoch 132, loss: 2.290022\n",
      "Epoch 133, loss: 2.289443\n",
      "Epoch 134, loss: 2.294438\n",
      "Epoch 135, loss: 2.291419\n",
      "Epoch 136, loss: 2.293466\n",
      "Epoch 137, loss: 2.289945\n",
      "Epoch 138, loss: 2.291152\n",
      "Epoch 139, loss: 2.292801\n",
      "Epoch 140, loss: 2.290344\n",
      "Epoch 141, loss: 2.290861\n",
      "Epoch 142, loss: 2.290068\n",
      "Epoch 143, loss: 2.293475\n",
      "Epoch 144, loss: 2.289236\n",
      "Epoch 145, loss: 2.291304\n",
      "Epoch 146, loss: 2.290402\n",
      "Epoch 147, loss: 2.286446\n",
      "Epoch 148, loss: 2.288848\n",
      "Epoch 149, loss: 2.288777\n",
      "Epoch 150, loss: 2.291477\n",
      "Epoch 151, loss: 2.285855\n",
      "Epoch 152, loss: 2.287389\n",
      "Epoch 153, loss: 2.291269\n",
      "Epoch 154, loss: 2.284541\n",
      "Epoch 155, loss: 2.287171\n",
      "Epoch 156, loss: 2.289078\n",
      "Epoch 157, loss: 2.287364\n",
      "Epoch 158, loss: 2.288763\n",
      "Epoch 159, loss: 2.287981\n",
      "Epoch 160, loss: 2.288023\n",
      "Epoch 161, loss: 2.292352\n",
      "Epoch 162, loss: 2.288241\n",
      "Epoch 163, loss: 2.286306\n",
      "Epoch 164, loss: 2.291034\n",
      "Epoch 165, loss: 2.284612\n",
      "Epoch 166, loss: 2.291189\n",
      "Epoch 167, loss: 2.284186\n",
      "Epoch 168, loss: 2.285489\n",
      "Epoch 169, loss: 2.285284\n",
      "Epoch 170, loss: 2.285561\n",
      "Epoch 171, loss: 2.286339\n",
      "Epoch 172, loss: 2.289177\n",
      "Epoch 173, loss: 2.288402\n",
      "Epoch 174, loss: 2.287816\n",
      "Epoch 175, loss: 2.287579\n",
      "Epoch 176, loss: 2.284685\n",
      "Epoch 177, loss: 2.282518\n",
      "Epoch 178, loss: 2.290574\n",
      "Epoch 179, loss: 2.292186\n",
      "Epoch 180, loss: 2.283190\n",
      "Epoch 181, loss: 2.288784\n",
      "Epoch 182, loss: 2.285091\n",
      "Epoch 183, loss: 2.286017\n",
      "Epoch 184, loss: 2.287550\n",
      "Epoch 185, loss: 2.289593\n",
      "Epoch 186, loss: 2.282529\n",
      "Epoch 187, loss: 2.284871\n",
      "Epoch 188, loss: 2.287857\n",
      "Epoch 189, loss: 2.283141\n",
      "Epoch 190, loss: 2.287441\n",
      "Epoch 191, loss: 2.282360\n",
      "Epoch 192, loss: 2.289572\n",
      "Epoch 193, loss: 2.284836\n",
      "Epoch 194, loss: 2.285441\n",
      "Epoch 195, loss: 2.285944\n",
      "Epoch 196, loss: 2.283469\n",
      "Epoch 197, loss: 2.287642\n",
      "Epoch 198, loss: 2.286929\n",
      "Epoch 199, loss: 2.286377\n",
      "Epoch 0, loss: 2.301888\n",
      "Epoch 1, loss: 2.302803\n",
      "Epoch 2, loss: 2.302804\n",
      "Epoch 3, loss: 2.302705\n",
      "Epoch 4, loss: 2.302649\n",
      "Epoch 5, loss: 2.303385\n",
      "Epoch 6, loss: 2.301822\n",
      "Epoch 7, loss: 2.303277\n",
      "Epoch 8, loss: 2.302536\n",
      "Epoch 9, loss: 2.302537\n",
      "Epoch 10, loss: 2.301641\n",
      "Epoch 11, loss: 2.302659\n",
      "Epoch 12, loss: 2.302785\n",
      "Epoch 13, loss: 2.302638\n",
      "Epoch 14, loss: 2.302911\n",
      "Epoch 15, loss: 2.301443\n",
      "Epoch 16, loss: 2.303244\n",
      "Epoch 17, loss: 2.303408\n",
      "Epoch 18, loss: 2.303059\n",
      "Epoch 19, loss: 2.302554\n",
      "Epoch 20, loss: 2.302286\n",
      "Epoch 21, loss: 2.303241\n",
      "Epoch 22, loss: 2.301972\n",
      "Epoch 23, loss: 2.302546\n",
      "Epoch 24, loss: 2.302033\n",
      "Epoch 25, loss: 2.302405\n",
      "Epoch 26, loss: 2.302516\n",
      "Epoch 27, loss: 2.302586\n",
      "Epoch 28, loss: 2.302126\n",
      "Epoch 29, loss: 2.302077\n",
      "Epoch 30, loss: 2.301932\n",
      "Epoch 31, loss: 2.302677\n",
      "Epoch 32, loss: 2.303515\n",
      "Epoch 33, loss: 2.302440\n",
      "Epoch 34, loss: 2.303137\n",
      "Epoch 35, loss: 2.302017\n",
      "Epoch 36, loss: 2.301864\n",
      "Epoch 37, loss: 2.302775\n",
      "Epoch 38, loss: 2.302884\n",
      "Epoch 39, loss: 2.301028\n",
      "Epoch 40, loss: 2.302069\n",
      "Epoch 41, loss: 2.301726\n",
      "Epoch 42, loss: 2.301906\n",
      "Epoch 43, loss: 2.302719\n",
      "Epoch 44, loss: 2.302164\n",
      "Epoch 45, loss: 2.301461\n",
      "Epoch 46, loss: 2.301460\n",
      "Epoch 47, loss: 2.302893\n",
      "Epoch 48, loss: 2.301526\n",
      "Epoch 49, loss: 2.302445\n",
      "Epoch 50, loss: 2.302178\n",
      "Epoch 51, loss: 2.301767\n",
      "Epoch 52, loss: 2.301269\n",
      "Epoch 53, loss: 2.303138\n",
      "Epoch 54, loss: 2.301491\n",
      "Epoch 55, loss: 2.300768\n",
      "Epoch 56, loss: 2.301981\n",
      "Epoch 57, loss: 2.301420\n",
      "Epoch 58, loss: 2.302664\n",
      "Epoch 59, loss: 2.302935\n",
      "Epoch 60, loss: 2.301230\n",
      "Epoch 61, loss: 2.301522\n",
      "Epoch 62, loss: 2.302775\n",
      "Epoch 63, loss: 2.302581\n",
      "Epoch 64, loss: 2.301936\n",
      "Epoch 65, loss: 2.301091\n",
      "Epoch 66, loss: 2.302935\n",
      "Epoch 67, loss: 2.303007\n",
      "Epoch 68, loss: 2.301400\n",
      "Epoch 69, loss: 2.302041\n",
      "Epoch 70, loss: 2.302342\n",
      "Epoch 71, loss: 2.301488\n",
      "Epoch 72, loss: 2.301926\n",
      "Epoch 73, loss: 2.302400\n",
      "Epoch 74, loss: 2.301022\n",
      "Epoch 75, loss: 2.301413\n",
      "Epoch 76, loss: 2.301474\n",
      "Epoch 77, loss: 2.301946\n",
      "Epoch 78, loss: 2.301585\n",
      "Epoch 79, loss: 2.300485\n",
      "Epoch 80, loss: 2.302481\n",
      "Epoch 81, loss: 2.301665\n",
      "Epoch 82, loss: 2.302147\n",
      "Epoch 83, loss: 2.301635\n",
      "Epoch 84, loss: 2.300948\n",
      "Epoch 85, loss: 2.301225\n",
      "Epoch 86, loss: 2.301106\n",
      "Epoch 87, loss: 2.302177\n",
      "Epoch 88, loss: 2.301870\n",
      "Epoch 89, loss: 2.302301\n",
      "Epoch 90, loss: 2.301189\n",
      "Epoch 91, loss: 2.301309\n",
      "Epoch 92, loss: 2.301718\n",
      "Epoch 93, loss: 2.302704\n",
      "Epoch 94, loss: 2.302304\n",
      "Epoch 95, loss: 2.301090\n",
      "Epoch 96, loss: 2.301631\n",
      "Epoch 97, loss: 2.302004\n",
      "Epoch 98, loss: 2.301276\n",
      "Epoch 99, loss: 2.301358\n",
      "Epoch 100, loss: 2.301265\n",
      "Epoch 101, loss: 2.302163\n",
      "Epoch 102, loss: 2.301310\n",
      "Epoch 103, loss: 2.301126\n",
      "Epoch 104, loss: 2.301277\n",
      "Epoch 105, loss: 2.302362\n",
      "Epoch 106, loss: 2.301593\n",
      "Epoch 107, loss: 2.301462\n",
      "Epoch 108, loss: 2.302452\n",
      "Epoch 109, loss: 2.302349\n",
      "Epoch 110, loss: 2.300532\n",
      "Epoch 111, loss: 2.300761\n",
      "Epoch 112, loss: 2.301019\n",
      "Epoch 113, loss: 2.300554\n",
      "Epoch 114, loss: 2.300401\n",
      "Epoch 115, loss: 2.301676\n",
      "Epoch 116, loss: 2.302088\n",
      "Epoch 117, loss: 2.301514\n",
      "Epoch 118, loss: 2.301605\n",
      "Epoch 119, loss: 2.301993\n",
      "Epoch 120, loss: 2.302446\n",
      "Epoch 121, loss: 2.303151\n",
      "Epoch 122, loss: 2.301022\n",
      "Epoch 123, loss: 2.301568\n",
      "Epoch 124, loss: 2.301296\n",
      "Epoch 125, loss: 2.302223\n",
      "Epoch 126, loss: 2.302829\n",
      "Epoch 127, loss: 2.302186\n",
      "Epoch 128, loss: 2.300724\n",
      "Epoch 129, loss: 2.300979\n",
      "Epoch 130, loss: 2.302279\n",
      "Epoch 131, loss: 2.300683\n",
      "Epoch 132, loss: 2.301268\n",
      "Epoch 133, loss: 2.301494\n",
      "Epoch 134, loss: 2.300601\n",
      "Epoch 135, loss: 2.301180\n",
      "Epoch 136, loss: 2.301353\n",
      "Epoch 137, loss: 2.300731\n",
      "Epoch 138, loss: 2.300765\n",
      "Epoch 139, loss: 2.301364\n",
      "Epoch 140, loss: 2.301738\n",
      "Epoch 141, loss: 2.299722\n",
      "Epoch 142, loss: 2.302753\n",
      "Epoch 143, loss: 2.301987\n",
      "Epoch 144, loss: 2.301019\n",
      "Epoch 145, loss: 2.301155\n",
      "Epoch 146, loss: 2.300136\n",
      "Epoch 147, loss: 2.300810\n",
      "Epoch 148, loss: 2.302389\n",
      "Epoch 149, loss: 2.301845\n",
      "Epoch 150, loss: 2.300617\n",
      "Epoch 151, loss: 2.300346\n",
      "Epoch 152, loss: 2.300793\n",
      "Epoch 153, loss: 2.300622\n",
      "Epoch 154, loss: 2.301759\n",
      "Epoch 155, loss: 2.300422\n",
      "Epoch 156, loss: 2.301191\n",
      "Epoch 157, loss: 2.300986\n",
      "Epoch 158, loss: 2.301513\n",
      "Epoch 159, loss: 2.300270\n",
      "Epoch 160, loss: 2.300344\n",
      "Epoch 161, loss: 2.300072\n",
      "Epoch 162, loss: 2.300239\n",
      "Epoch 163, loss: 2.299933\n",
      "Epoch 164, loss: 2.301690\n",
      "Epoch 165, loss: 2.300311\n",
      "Epoch 166, loss: 2.301122\n",
      "Epoch 167, loss: 2.300821\n",
      "Epoch 168, loss: 2.301353\n",
      "Epoch 169, loss: 2.301315\n",
      "Epoch 170, loss: 2.300395\n",
      "Epoch 171, loss: 2.300856\n",
      "Epoch 172, loss: 2.299837\n",
      "Epoch 173, loss: 2.301074\n",
      "Epoch 174, loss: 2.300972\n",
      "Epoch 175, loss: 2.299189\n",
      "Epoch 176, loss: 2.301072\n",
      "Epoch 177, loss: 2.300158\n",
      "Epoch 178, loss: 2.299067\n",
      "Epoch 179, loss: 2.300887\n",
      "Epoch 180, loss: 2.301236\n",
      "Epoch 181, loss: 2.301755\n",
      "Epoch 182, loss: 2.300947\n",
      "Epoch 183, loss: 2.301529\n",
      "Epoch 184, loss: 2.301313\n",
      "Epoch 185, loss: 2.299597\n",
      "Epoch 186, loss: 2.300958\n",
      "Epoch 187, loss: 2.301047\n",
      "Epoch 188, loss: 2.300277\n",
      "Epoch 189, loss: 2.299644\n",
      "Epoch 190, loss: 2.300806\n",
      "Epoch 191, loss: 2.300462\n",
      "Epoch 192, loss: 2.298689\n",
      "Epoch 193, loss: 2.302019\n",
      "Epoch 194, loss: 2.301071\n",
      "Epoch 195, loss: 2.300725\n",
      "Epoch 196, loss: 2.301066\n",
      "Epoch 197, loss: 2.301008\n",
      "Epoch 198, loss: 2.300919\n",
      "Epoch 199, loss: 2.300451\n",
      "Epoch 0, loss: 2.302773\n",
      "Epoch 1, loss: 2.303589\n",
      "Epoch 2, loss: 2.302858\n",
      "Epoch 3, loss: 2.301939\n",
      "Epoch 4, loss: 2.301979\n",
      "Epoch 5, loss: 2.302073\n",
      "Epoch 6, loss: 2.302608\n",
      "Epoch 7, loss: 2.302581\n",
      "Epoch 8, loss: 2.302897\n",
      "Epoch 9, loss: 2.302204\n",
      "Epoch 10, loss: 2.302497\n",
      "Epoch 11, loss: 2.302610\n",
      "Epoch 12, loss: 2.302573\n",
      "Epoch 13, loss: 2.302658\n",
      "Epoch 14, loss: 2.302786\n",
      "Epoch 15, loss: 2.302962\n",
      "Epoch 16, loss: 2.302433\n",
      "Epoch 17, loss: 2.302875\n",
      "Epoch 18, loss: 2.303079\n",
      "Epoch 19, loss: 2.302247\n",
      "Epoch 20, loss: 2.302802\n",
      "Epoch 21, loss: 2.302950\n",
      "Epoch 22, loss: 2.302000\n",
      "Epoch 23, loss: 2.302969\n",
      "Epoch 24, loss: 2.301207\n",
      "Epoch 25, loss: 2.302632\n",
      "Epoch 26, loss: 2.303840\n",
      "Epoch 27, loss: 2.302147\n",
      "Epoch 28, loss: 2.302499\n",
      "Epoch 29, loss: 2.302526\n",
      "Epoch 30, loss: 2.302398\n",
      "Epoch 31, loss: 2.301774\n",
      "Epoch 32, loss: 2.302388\n",
      "Epoch 33, loss: 2.302934\n",
      "Epoch 34, loss: 2.301595\n",
      "Epoch 35, loss: 2.301918\n",
      "Epoch 36, loss: 2.302423\n",
      "Epoch 37, loss: 2.302182\n",
      "Epoch 38, loss: 2.303679\n",
      "Epoch 39, loss: 2.302293\n",
      "Epoch 40, loss: 2.301506\n",
      "Epoch 41, loss: 2.302201\n",
      "Epoch 42, loss: 2.302302\n",
      "Epoch 43, loss: 2.302936\n",
      "Epoch 44, loss: 2.301978\n",
      "Epoch 45, loss: 2.302600\n",
      "Epoch 46, loss: 2.302493\n",
      "Epoch 47, loss: 2.301949\n",
      "Epoch 48, loss: 2.301099\n",
      "Epoch 49, loss: 2.301902\n",
      "Epoch 50, loss: 2.301295\n",
      "Epoch 51, loss: 2.302327\n",
      "Epoch 52, loss: 2.302361\n",
      "Epoch 53, loss: 2.301944\n",
      "Epoch 54, loss: 2.301648\n",
      "Epoch 55, loss: 2.300560\n",
      "Epoch 56, loss: 2.301815\n",
      "Epoch 57, loss: 2.301108\n",
      "Epoch 58, loss: 2.301674\n",
      "Epoch 59, loss: 2.301081\n",
      "Epoch 60, loss: 2.301973\n",
      "Epoch 61, loss: 2.301390\n",
      "Epoch 62, loss: 2.301985\n",
      "Epoch 63, loss: 2.301817\n",
      "Epoch 64, loss: 2.301854\n",
      "Epoch 65, loss: 2.301503\n",
      "Epoch 66, loss: 2.302716\n",
      "Epoch 67, loss: 2.302286\n",
      "Epoch 68, loss: 2.302478\n",
      "Epoch 69, loss: 2.301609\n",
      "Epoch 70, loss: 2.302362\n",
      "Epoch 71, loss: 2.300996\n",
      "Epoch 72, loss: 2.301761\n",
      "Epoch 73, loss: 2.301077\n",
      "Epoch 74, loss: 2.302502\n",
      "Epoch 75, loss: 2.301074\n",
      "Epoch 76, loss: 2.303046\n",
      "Epoch 77, loss: 2.301484\n",
      "Epoch 78, loss: 2.301561\n",
      "Epoch 79, loss: 2.302319\n",
      "Epoch 80, loss: 2.301274\n",
      "Epoch 81, loss: 2.303301\n",
      "Epoch 82, loss: 2.302512\n",
      "Epoch 83, loss: 2.301080\n",
      "Epoch 84, loss: 2.301477\n",
      "Epoch 85, loss: 2.302462\n",
      "Epoch 86, loss: 2.302495\n",
      "Epoch 87, loss: 2.301829\n",
      "Epoch 88, loss: 2.301764\n",
      "Epoch 89, loss: 2.301190\n",
      "Epoch 90, loss: 2.301562\n",
      "Epoch 91, loss: 2.301842\n",
      "Epoch 92, loss: 2.299485\n",
      "Epoch 93, loss: 2.302126\n",
      "Epoch 94, loss: 2.301478\n",
      "Epoch 95, loss: 2.301588\n",
      "Epoch 96, loss: 2.302086\n",
      "Epoch 97, loss: 2.301243\n",
      "Epoch 98, loss: 2.301161\n",
      "Epoch 99, loss: 2.301146\n",
      "Epoch 100, loss: 2.300876\n",
      "Epoch 101, loss: 2.302186\n",
      "Epoch 102, loss: 2.302036\n",
      "Epoch 103, loss: 2.301431\n",
      "Epoch 104, loss: 2.300859\n",
      "Epoch 105, loss: 2.301374\n",
      "Epoch 106, loss: 2.300591\n",
      "Epoch 107, loss: 2.301434\n",
      "Epoch 108, loss: 2.301639\n",
      "Epoch 109, loss: 2.302314\n",
      "Epoch 110, loss: 2.301965\n",
      "Epoch 111, loss: 2.300751\n",
      "Epoch 112, loss: 2.301107\n",
      "Epoch 113, loss: 2.300309\n",
      "Epoch 114, loss: 2.301958\n",
      "Epoch 115, loss: 2.300573\n",
      "Epoch 116, loss: 2.301583\n",
      "Epoch 117, loss: 2.301448\n",
      "Epoch 118, loss: 2.301840\n",
      "Epoch 119, loss: 2.300277\n",
      "Epoch 120, loss: 2.301396\n",
      "Epoch 121, loss: 2.301159\n",
      "Epoch 122, loss: 2.300103\n",
      "Epoch 123, loss: 2.300664\n",
      "Epoch 124, loss: 2.301787\n",
      "Epoch 125, loss: 2.300443\n",
      "Epoch 126, loss: 2.302484\n",
      "Epoch 127, loss: 2.302102\n",
      "Epoch 128, loss: 2.301871\n",
      "Epoch 129, loss: 2.301783\n",
      "Epoch 130, loss: 2.300427\n",
      "Epoch 131, loss: 2.300956\n",
      "Epoch 132, loss: 2.301135\n",
      "Epoch 133, loss: 2.300781\n",
      "Epoch 134, loss: 2.301626\n",
      "Epoch 135, loss: 2.300571\n",
      "Epoch 136, loss: 2.301812\n",
      "Epoch 137, loss: 2.302302\n",
      "Epoch 138, loss: 2.301425\n",
      "Epoch 139, loss: 2.301056\n",
      "Epoch 140, loss: 2.302710\n",
      "Epoch 141, loss: 2.300649\n",
      "Epoch 142, loss: 2.302937\n",
      "Epoch 143, loss: 2.300785\n",
      "Epoch 144, loss: 2.300361\n",
      "Epoch 145, loss: 2.301800\n",
      "Epoch 146, loss: 2.301052\n",
      "Epoch 147, loss: 2.300450\n",
      "Epoch 148, loss: 2.300544\n",
      "Epoch 149, loss: 2.300557\n",
      "Epoch 150, loss: 2.300807\n",
      "Epoch 151, loss: 2.301574\n",
      "Epoch 152, loss: 2.300712\n",
      "Epoch 153, loss: 2.301307\n",
      "Epoch 154, loss: 2.300349\n",
      "Epoch 155, loss: 2.299678\n",
      "Epoch 156, loss: 2.301463\n",
      "Epoch 157, loss: 2.300608\n",
      "Epoch 158, loss: 2.300963\n",
      "Epoch 159, loss: 2.299700\n",
      "Epoch 160, loss: 2.299943\n",
      "Epoch 161, loss: 2.300622\n",
      "Epoch 162, loss: 2.300970\n",
      "Epoch 163, loss: 2.300802\n",
      "Epoch 164, loss: 2.299677\n",
      "Epoch 165, loss: 2.300947\n",
      "Epoch 166, loss: 2.301885\n",
      "Epoch 167, loss: 2.300842\n",
      "Epoch 168, loss: 2.300447\n",
      "Epoch 169, loss: 2.301285\n",
      "Epoch 170, loss: 2.301017\n",
      "Epoch 171, loss: 2.299513\n",
      "Epoch 172, loss: 2.300843\n",
      "Epoch 173, loss: 2.301203\n",
      "Epoch 174, loss: 2.301571\n",
      "Epoch 175, loss: 2.301499\n",
      "Epoch 176, loss: 2.300016\n",
      "Epoch 177, loss: 2.300053\n",
      "Epoch 178, loss: 2.300988\n",
      "Epoch 179, loss: 2.301232\n",
      "Epoch 180, loss: 2.300993\n",
      "Epoch 181, loss: 2.301097\n",
      "Epoch 182, loss: 2.300252\n",
      "Epoch 183, loss: 2.299908\n",
      "Epoch 184, loss: 2.301781\n",
      "Epoch 185, loss: 2.300742\n",
      "Epoch 186, loss: 2.300216\n",
      "Epoch 187, loss: 2.299424\n",
      "Epoch 188, loss: 2.300391\n",
      "Epoch 189, loss: 2.301343\n",
      "Epoch 190, loss: 2.299797\n",
      "Epoch 191, loss: 2.299879\n",
      "Epoch 192, loss: 2.300926\n",
      "Epoch 193, loss: 2.300967\n",
      "Epoch 194, loss: 2.300133\n",
      "Epoch 195, loss: 2.300316\n",
      "Epoch 196, loss: 2.301341\n",
      "Epoch 197, loss: 2.302406\n",
      "Epoch 198, loss: 2.300113\n",
      "Epoch 199, loss: 2.300887\n",
      "Epoch 0, loss: 2.302133\n",
      "Epoch 1, loss: 2.302946\n",
      "Epoch 2, loss: 2.302879\n",
      "Epoch 3, loss: 2.303658\n",
      "Epoch 4, loss: 2.303317\n",
      "Epoch 5, loss: 2.301630\n",
      "Epoch 6, loss: 2.301660\n",
      "Epoch 7, loss: 2.302037\n",
      "Epoch 8, loss: 2.302724\n",
      "Epoch 9, loss: 2.301360\n",
      "Epoch 10, loss: 2.302859\n",
      "Epoch 11, loss: 2.302945\n",
      "Epoch 12, loss: 2.302742\n",
      "Epoch 13, loss: 2.301668\n",
      "Epoch 14, loss: 2.301769\n",
      "Epoch 15, loss: 2.302659\n",
      "Epoch 16, loss: 2.302973\n",
      "Epoch 17, loss: 2.301399\n",
      "Epoch 18, loss: 2.302478\n",
      "Epoch 19, loss: 2.302659\n",
      "Epoch 20, loss: 2.302796\n",
      "Epoch 21, loss: 2.302164\n",
      "Epoch 22, loss: 2.302594\n",
      "Epoch 23, loss: 2.303413\n",
      "Epoch 24, loss: 2.302766\n",
      "Epoch 25, loss: 2.302437\n",
      "Epoch 26, loss: 2.302886\n",
      "Epoch 27, loss: 2.302055\n",
      "Epoch 28, loss: 2.302725\n",
      "Epoch 29, loss: 2.302417\n",
      "Epoch 30, loss: 2.301905\n",
      "Epoch 31, loss: 2.302062\n",
      "Epoch 32, loss: 2.302047\n",
      "Epoch 33, loss: 2.301432\n",
      "Epoch 34, loss: 2.300573\n",
      "Epoch 35, loss: 2.301218\n",
      "Epoch 36, loss: 2.302336\n",
      "Epoch 37, loss: 2.302672\n",
      "Epoch 38, loss: 2.301824\n",
      "Epoch 39, loss: 2.301961\n",
      "Epoch 40, loss: 2.301638\n",
      "Epoch 41, loss: 2.302695\n",
      "Epoch 42, loss: 2.302018\n",
      "Epoch 43, loss: 2.302044\n",
      "Epoch 44, loss: 2.301666\n",
      "Epoch 45, loss: 2.301678\n",
      "Epoch 46, loss: 2.301064\n",
      "Epoch 47, loss: 2.302460\n",
      "Epoch 48, loss: 2.301307\n",
      "Epoch 49, loss: 2.302604\n",
      "Epoch 50, loss: 2.302322\n",
      "Epoch 51, loss: 2.301969\n",
      "Epoch 52, loss: 2.302229\n",
      "Epoch 53, loss: 2.301945\n",
      "Epoch 54, loss: 2.301427\n",
      "Epoch 55, loss: 2.301778\n",
      "Epoch 56, loss: 2.302620\n",
      "Epoch 57, loss: 2.302596\n",
      "Epoch 58, loss: 2.301650\n",
      "Epoch 59, loss: 2.300870\n",
      "Epoch 60, loss: 2.302028\n",
      "Epoch 61, loss: 2.301341\n",
      "Epoch 62, loss: 2.302473\n",
      "Epoch 63, loss: 2.302525\n",
      "Epoch 64, loss: 2.301349\n",
      "Epoch 65, loss: 2.302038\n",
      "Epoch 66, loss: 2.301373\n",
      "Epoch 67, loss: 2.302054\n",
      "Epoch 68, loss: 2.302716\n",
      "Epoch 69, loss: 2.302699\n",
      "Epoch 70, loss: 2.301188\n",
      "Epoch 71, loss: 2.301854\n",
      "Epoch 72, loss: 2.300227\n",
      "Epoch 73, loss: 2.302826\n",
      "Epoch 74, loss: 2.302543\n",
      "Epoch 75, loss: 2.302170\n",
      "Epoch 76, loss: 2.301719\n",
      "Epoch 77, loss: 2.302178\n",
      "Epoch 78, loss: 2.301143\n",
      "Epoch 79, loss: 2.301661\n",
      "Epoch 80, loss: 2.301392\n",
      "Epoch 81, loss: 2.302269\n",
      "Epoch 82, loss: 2.301208\n",
      "Epoch 83, loss: 2.302121\n",
      "Epoch 84, loss: 2.301947\n",
      "Epoch 85, loss: 2.301060\n",
      "Epoch 86, loss: 2.301999\n",
      "Epoch 87, loss: 2.301576\n",
      "Epoch 88, loss: 2.302010\n",
      "Epoch 89, loss: 2.302051\n",
      "Epoch 90, loss: 2.303637\n",
      "Epoch 91, loss: 2.301180\n",
      "Epoch 92, loss: 2.302559\n",
      "Epoch 93, loss: 2.301275\n",
      "Epoch 94, loss: 2.302481\n",
      "Epoch 95, loss: 2.300723\n",
      "Epoch 96, loss: 2.302065\n",
      "Epoch 97, loss: 2.301039\n",
      "Epoch 98, loss: 2.302193\n",
      "Epoch 99, loss: 2.300702\n",
      "Epoch 100, loss: 2.300072\n",
      "Epoch 101, loss: 2.301111\n",
      "Epoch 102, loss: 2.300737\n",
      "Epoch 103, loss: 2.302365\n",
      "Epoch 104, loss: 2.302458\n",
      "Epoch 105, loss: 2.301854\n",
      "Epoch 106, loss: 2.300969\n",
      "Epoch 107, loss: 2.301381\n",
      "Epoch 108, loss: 2.301477\n",
      "Epoch 109, loss: 2.301073\n",
      "Epoch 110, loss: 2.300156\n",
      "Epoch 111, loss: 2.302292\n",
      "Epoch 112, loss: 2.301564\n",
      "Epoch 113, loss: 2.300803\n",
      "Epoch 114, loss: 2.301155\n",
      "Epoch 115, loss: 2.300939\n",
      "Epoch 116, loss: 2.302361\n",
      "Epoch 117, loss: 2.301491\n",
      "Epoch 118, loss: 2.301171\n",
      "Epoch 119, loss: 2.301956\n",
      "Epoch 120, loss: 2.301783\n",
      "Epoch 121, loss: 2.302048\n",
      "Epoch 122, loss: 2.301398\n",
      "Epoch 123, loss: 2.301289\n",
      "Epoch 124, loss: 2.300231\n",
      "Epoch 125, loss: 2.301546\n",
      "Epoch 126, loss: 2.300554\n",
      "Epoch 127, loss: 2.300114\n",
      "Epoch 128, loss: 2.302071\n",
      "Epoch 129, loss: 2.300844\n",
      "Epoch 130, loss: 2.300342\n",
      "Epoch 131, loss: 2.300692\n",
      "Epoch 132, loss: 2.300953\n",
      "Epoch 133, loss: 2.301220\n",
      "Epoch 134, loss: 2.301030\n",
      "Epoch 135, loss: 2.301118\n",
      "Epoch 136, loss: 2.300377\n",
      "Epoch 137, loss: 2.302856\n",
      "Epoch 138, loss: 2.301435\n",
      "Epoch 139, loss: 2.300849\n",
      "Epoch 140, loss: 2.302030\n",
      "Epoch 141, loss: 2.301404\n",
      "Epoch 142, loss: 2.301772\n",
      "Epoch 143, loss: 2.300886\n",
      "Epoch 144, loss: 2.299714\n",
      "Epoch 145, loss: 2.300652\n",
      "Epoch 146, loss: 2.301717\n",
      "Epoch 147, loss: 2.302366\n",
      "Epoch 148, loss: 2.300469\n",
      "Epoch 149, loss: 2.301069\n",
      "Epoch 150, loss: 2.300997\n",
      "Epoch 151, loss: 2.299688\n",
      "Epoch 152, loss: 2.301764\n",
      "Epoch 153, loss: 2.300720\n",
      "Epoch 154, loss: 2.301394\n",
      "Epoch 155, loss: 2.300619\n",
      "Epoch 156, loss: 2.301488\n",
      "Epoch 157, loss: 2.300965\n",
      "Epoch 158, loss: 2.301849\n",
      "Epoch 159, loss: 2.300929\n",
      "Epoch 160, loss: 2.300965\n",
      "Epoch 161, loss: 2.302019\n",
      "Epoch 162, loss: 2.300756\n",
      "Epoch 163, loss: 2.299450\n",
      "Epoch 164, loss: 2.300041\n",
      "Epoch 165, loss: 2.300019\n",
      "Epoch 166, loss: 2.301070\n",
      "Epoch 167, loss: 2.300386\n",
      "Epoch 168, loss: 2.299439\n",
      "Epoch 169, loss: 2.300282\n",
      "Epoch 170, loss: 2.300897\n",
      "Epoch 171, loss: 2.301880\n",
      "Epoch 172, loss: 2.300200\n",
      "Epoch 173, loss: 2.300663\n",
      "Epoch 174, loss: 2.300318\n",
      "Epoch 175, loss: 2.302247\n",
      "Epoch 176, loss: 2.300262\n",
      "Epoch 177, loss: 2.300075\n",
      "Epoch 178, loss: 2.299128\n",
      "Epoch 179, loss: 2.301732\n",
      "Epoch 180, loss: 2.298706\n",
      "Epoch 181, loss: 2.300820\n",
      "Epoch 182, loss: 2.301458\n",
      "Epoch 183, loss: 2.300143\n",
      "Epoch 184, loss: 2.301892\n",
      "Epoch 185, loss: 2.300427\n",
      "Epoch 186, loss: 2.300392\n",
      "Epoch 187, loss: 2.300208\n",
      "Epoch 188, loss: 2.301508\n",
      "Epoch 189, loss: 2.301971\n",
      "Epoch 190, loss: 2.300396\n",
      "Epoch 191, loss: 2.301389\n",
      "Epoch 192, loss: 2.300605\n",
      "Epoch 193, loss: 2.300721\n",
      "Epoch 194, loss: 2.299770\n",
      "Epoch 195, loss: 2.299451\n",
      "Epoch 196, loss: 2.300245\n",
      "Epoch 197, loss: 2.301016\n",
      "Epoch 198, loss: 2.301587\n",
      "Epoch 199, loss: 2.299325\n",
      "hyper-parameters: lr = 1e-05, rs = 1e-06\n",
      "best validation accuracy achieved: 0.230000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        loss_history = classifier.fit(train_X, train_y, epochs=num_epochs,\n",
    "                                      learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "            best_lr, best_rs = lr, rs\n",
    "\n",
    "print('hyper-parameters: lr = {}, rs = {}'.format(lr, rs))\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.195000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
