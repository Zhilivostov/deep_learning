{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "#!pip3 -qq install torch==0.4.1\n",
    "#!pip3 -qq install bokeh==0.13.0\n",
    "#!pip3 -qq install gensim==3.6.0\n",
    "#!pip3 -qq install nltk\n",
    "#!pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'NUM', 'X', 'ADV', 'DET', '.', 'PRT', 'PRON', 'VERB', 'CONJ', 'NOUN', 'ADP', 'ADJ'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_12336\\3805302136.py:6: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_12336\\648162845.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_12336\\28852427.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Word embedding layer\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(word_emb_dim, \n",
    "                           lstm_hidden_dim, \n",
    "                           num_layers=lstm_layers_count,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        # Linear layer to map from LSTM hidden state to tag space\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Get embeddings for the input tokens\n",
    "        embeds = self.word_embeddings(inputs)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        \n",
    "        # Convert LSTM outputs to tag space\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        \n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "predicted_tags = torch.argmax(logits, dim=-1)  # shape: (batch_size, sequence_length)\n",
    "correct = (predicted_tags == y_batch).sum().item()\n",
    "total = y_batch.numel()\n",
    "accuracy = correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Need to reshape logits and labels for CrossEntropyLoss:\n",
    "# (batch_size, seq_len, num_tags) -> (batch_size*seq_len, num_tags)\n",
    "# (batch_size, seq_len) -> (batch_size*seq_len)\n",
    "loss = criterion(logits.view(-1, logits.size(-1)), y_batch.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), y_batch.view(-1))\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                #cur_correct_count, cur_sum_count = <calc accuracy>\n",
    "                predicted_tags = torch.argmax(logits, dim=-1)\n",
    "                cur_correct_count = (predicted_tags == y_batch).sum().item()\n",
    "                cur_sum_count = y_batch.numel()\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.34254, Accuracy = 90.04%: 100%|██████████| 572/572 [00:10<00:00, 52.52it/s]\n",
      "[1 / 50]   Val: Loss = 0.14410, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 24.04it/s]\n",
      "[2 / 50] Train: Loss = 0.11485, Accuracy = 96.40%: 100%|██████████| 572/572 [00:10<00:00, 53.91it/s]\n",
      "[2 / 50]   Val: Loss = 0.11823, Accuracy = 97.80%: 100%|██████████| 13/13 [00:00<00:00, 23.57it/s]\n",
      "[3 / 50] Train: Loss = 0.08166, Accuracy = 97.37%: 100%|██████████| 572/572 [00:10<00:00, 53.92it/s]\n",
      "[3 / 50]   Val: Loss = 0.10843, Accuracy = 98.14%: 100%|██████████| 13/13 [00:00<00:00, 23.57it/s]\n",
      "[4 / 50] Train: Loss = 0.06655, Accuracy = 97.79%: 100%|██████████| 572/572 [00:10<00:00, 53.65it/s]\n",
      "[4 / 50]   Val: Loss = 0.11201, Accuracy = 98.26%: 100%|██████████| 13/13 [00:00<00:00, 23.71it/s]\n",
      "[5 / 50] Train: Loss = 0.05727, Accuracy = 98.03%: 100%|██████████| 572/572 [00:10<00:00, 53.70it/s]\n",
      "[5 / 50]   Val: Loss = 0.10273, Accuracy = 98.40%: 100%|██████████| 13/13 [00:00<00:00, 22.35it/s]\n",
      "[6 / 50] Train: Loss = 0.05079, Accuracy = 98.19%: 100%|██████████| 572/572 [00:10<00:00, 53.33it/s]\n",
      "[6 / 50]   Val: Loss = 0.11095, Accuracy = 98.37%: 100%|██████████| 13/13 [00:00<00:00, 22.75it/s]\n",
      "[7 / 50] Train: Loss = 0.04696, Accuracy = 98.29%: 100%|██████████| 572/572 [00:10<00:00, 53.35it/s]\n",
      "[7 / 50]   Val: Loss = 0.10860, Accuracy = 98.45%: 100%|██████████| 13/13 [00:00<00:00, 23.01it/s]\n",
      "[8 / 50] Train: Loss = 0.04331, Accuracy = 98.38%: 100%|██████████| 572/572 [00:10<00:00, 52.64it/s]\n",
      "[8 / 50]   Val: Loss = 0.11043, Accuracy = 98.45%: 100%|██████████| 13/13 [00:00<00:00, 23.35it/s]\n",
      "[9 / 50] Train: Loss = 0.04082, Accuracy = 98.44%: 100%|██████████| 572/572 [00:11<00:00, 51.54it/s]\n",
      "[9 / 50]   Val: Loss = 0.11111, Accuracy = 98.50%: 100%|██████████| 13/13 [00:00<00:00, 22.36it/s]\n",
      "[10 / 50] Train: Loss = 0.03902, Accuracy = 98.49%: 100%|██████████| 572/572 [00:11<00:00, 50.95it/s]\n",
      "[10 / 50]   Val: Loss = 0.10555, Accuracy = 98.56%: 100%|██████████| 13/13 [00:00<00:00, 22.35it/s]\n",
      "[11 / 50] Train: Loss = 0.03763, Accuracy = 98.53%: 100%|██████████| 572/572 [00:11<00:00, 51.11it/s]\n",
      "[11 / 50]   Val: Loss = 0.10740, Accuracy = 98.54%: 100%|██████████| 13/13 [00:00<00:00, 21.76it/s]\n",
      "[12 / 50] Train: Loss = 0.03661, Accuracy = 98.54%: 100%|██████████| 572/572 [00:11<00:00, 51.09it/s]\n",
      "[12 / 50]   Val: Loss = 0.11146, Accuracy = 98.52%: 100%|██████████| 13/13 [00:00<00:00, 22.34it/s]\n",
      "[13 / 50] Train: Loss = 0.03570, Accuracy = 98.57%: 100%|██████████| 572/572 [00:11<00:00, 50.82it/s]\n",
      "[13 / 50]   Val: Loss = 0.11108, Accuracy = 98.52%: 100%|██████████| 13/13 [00:00<00:00, 22.32it/s]\n",
      "[14 / 50] Train: Loss = 0.03470, Accuracy = 98.60%: 100%|██████████| 572/572 [00:11<00:00, 50.33it/s]\n",
      "[14 / 50]   Val: Loss = 0.12309, Accuracy = 98.43%: 100%|██████████| 13/13 [00:00<00:00, 23.81it/s]\n",
      "[15 / 50] Train: Loss = 0.03442, Accuracy = 98.60%: 100%|██████████| 572/572 [00:11<00:00, 50.51it/s]\n",
      "[15 / 50]   Val: Loss = 0.12050, Accuracy = 98.47%: 100%|██████████| 13/13 [00:00<00:00, 22.34it/s]\n",
      "[16 / 50] Train: Loss = 0.03386, Accuracy = 98.61%: 100%|██████████| 572/572 [00:11<00:00, 49.85it/s]\n",
      "[16 / 50]   Val: Loss = 0.12280, Accuracy = 98.42%: 100%|██████████| 13/13 [00:00<00:00, 23.52it/s]\n",
      "[17 / 50] Train: Loss = 0.03365, Accuracy = 98.61%: 100%|██████████| 572/572 [00:11<00:00, 49.83it/s]\n",
      "[17 / 50]   Val: Loss = 0.11133, Accuracy = 98.58%: 100%|██████████| 13/13 [00:00<00:00, 21.58it/s]\n",
      "[18 / 50] Train: Loss = 0.03306, Accuracy = 98.63%: 100%|██████████| 572/572 [00:11<00:00, 48.79it/s]\n",
      "[18 / 50]   Val: Loss = 0.11862, Accuracy = 98.51%: 100%|██████████| 13/13 [00:00<00:00, 20.60it/s]\n",
      "[19 / 50] Train: Loss = 0.03277, Accuracy = 98.64%: 100%|██████████| 572/572 [00:13<00:00, 42.82it/s]\n",
      "[19 / 50]   Val: Loss = 0.12340, Accuracy = 98.44%: 100%|██████████| 13/13 [00:00<00:00, 22.15it/s]\n",
      "[20 / 50] Train: Loss = 0.03249, Accuracy = 98.64%: 100%|██████████| 572/572 [00:11<00:00, 47.91it/s]\n",
      "[20 / 50]   Val: Loss = 0.11980, Accuracy = 98.54%: 100%|██████████| 13/13 [00:00<00:00, 21.52it/s]\n",
      "[21 / 50] Train: Loss = 0.03255, Accuracy = 98.64%: 100%|██████████| 572/572 [00:11<00:00, 48.64it/s]\n",
      "[21 / 50]   Val: Loss = 0.12077, Accuracy = 98.55%: 100%|██████████| 13/13 [00:00<00:00, 21.90it/s]\n",
      "[22 / 50] Train: Loss = 0.03229, Accuracy = 98.65%: 100%|██████████| 572/572 [00:11<00:00, 48.52it/s]\n",
      "[22 / 50]   Val: Loss = 0.12640, Accuracy = 98.49%: 100%|██████████| 13/13 [00:00<00:00, 22.28it/s]\n",
      "[23 / 50] Train: Loss = 0.03202, Accuracy = 98.66%: 100%|██████████| 572/572 [00:11<00:00, 48.17it/s]\n",
      "[23 / 50]   Val: Loss = 0.13934, Accuracy = 98.36%: 100%|██████████| 13/13 [00:00<00:00, 24.78it/s]\n",
      "[24 / 50] Train: Loss = 0.03188, Accuracy = 98.67%: 100%|██████████| 572/572 [00:11<00:00, 48.03it/s]\n",
      "[24 / 50]   Val: Loss = 0.12885, Accuracy = 98.47%: 100%|██████████| 13/13 [00:00<00:00, 22.41it/s]\n",
      "[25 / 50] Train: Loss = 0.03207, Accuracy = 98.65%: 100%|██████████| 572/572 [00:11<00:00, 47.69it/s]\n",
      "[25 / 50]   Val: Loss = 0.12721, Accuracy = 98.58%: 100%|██████████| 13/13 [00:00<00:00, 20.68it/s]\n",
      "[26 / 50] Train: Loss = 0.03153, Accuracy = 98.67%: 100%|██████████| 572/572 [00:12<00:00, 46.98it/s]\n",
      "[26 / 50]   Val: Loss = 0.13131, Accuracy = 98.47%: 100%|██████████| 13/13 [00:00<00:00, 21.98it/s]\n",
      "[27 / 50] Train: Loss = 0.03185, Accuracy = 98.66%: 100%|██████████| 572/572 [00:12<00:00, 47.08it/s]\n",
      "[27 / 50]   Val: Loss = 0.14173, Accuracy = 98.46%: 100%|██████████| 13/13 [00:00<00:00, 22.44it/s]\n",
      "[28 / 50] Train: Loss = 0.03169, Accuracy = 98.66%: 100%|██████████| 572/572 [00:12<00:00, 46.60it/s]\n",
      "[28 / 50]   Val: Loss = 0.13225, Accuracy = 98.52%: 100%|██████████| 13/13 [00:00<00:00, 21.28it/s]\n",
      "[29 / 50] Train: Loss = 0.03158, Accuracy = 98.67%: 100%|██████████| 572/572 [00:12<00:00, 46.35it/s]\n",
      "[29 / 50]   Val: Loss = 0.12862, Accuracy = 98.57%: 100%|██████████| 13/13 [00:00<00:00, 20.62it/s]\n",
      "[30 / 50] Train: Loss = 0.03144, Accuracy = 98.67%: 100%|██████████| 572/572 [00:12<00:00, 46.49it/s]\n",
      "[30 / 50]   Val: Loss = 0.13814, Accuracy = 98.53%: 100%|██████████| 13/13 [00:00<00:00, 20.40it/s]\n",
      "[31 / 50] Train: Loss = 0.03163, Accuracy = 98.66%: 100%|██████████| 572/572 [00:12<00:00, 46.12it/s]\n",
      "[31 / 50]   Val: Loss = 0.14098, Accuracy = 98.53%: 100%|██████████| 13/13 [00:00<00:00, 21.07it/s]\n",
      "[32 / 50] Train: Loss = 0.03164, Accuracy = 98.67%: 100%|██████████| 572/572 [00:12<00:00, 45.71it/s]\n",
      "[32 / 50]   Val: Loss = 0.13704, Accuracy = 98.54%: 100%|██████████| 13/13 [00:00<00:00, 21.47it/s]\n",
      "[33 / 50] Train: Loss = 0.03127, Accuracy = 98.68%: 100%|██████████| 572/572 [00:12<00:00, 45.71it/s]\n",
      "[33 / 50]   Val: Loss = 0.14336, Accuracy = 98.53%: 100%|██████████| 13/13 [00:00<00:00, 20.83it/s]\n",
      "[34 / 50] Train: Loss = 0.03120, Accuracy = 98.68%: 100%|██████████| 572/572 [00:12<00:00, 45.37it/s]\n",
      "[34 / 50]   Val: Loss = 0.14217, Accuracy = 98.50%: 100%|██████████| 13/13 [00:00<00:00, 22.92it/s]\n",
      "[35 / 50] Train: Loss = 0.03116, Accuracy = 98.68%: 100%|██████████| 572/572 [00:12<00:00, 45.41it/s]\n",
      "[35 / 50]   Val: Loss = 0.15613, Accuracy = 98.48%: 100%|██████████| 13/13 [00:00<00:00, 21.84it/s]\n",
      "[36 / 50] Train: Loss = 0.03137, Accuracy = 98.67%: 100%|██████████| 572/572 [00:12<00:00, 45.52it/s]\n",
      "[36 / 50]   Val: Loss = 0.14483, Accuracy = 98.50%: 100%|██████████| 13/13 [00:00<00:00, 21.53it/s]\n",
      "[37 / 50] Train: Loss = 0.03142, Accuracy = 98.67%: 100%|██████████| 572/572 [00:12<00:00, 45.44it/s]\n",
      "[37 / 50]   Val: Loss = 0.14856, Accuracy = 98.50%: 100%|██████████| 13/13 [00:00<00:00, 20.65it/s]\n",
      "[38 / 50] Train: Loss = 0.03122, Accuracy = 98.68%: 100%|██████████| 572/572 [00:12<00:00, 45.28it/s]\n",
      "[38 / 50]   Val: Loss = 0.14442, Accuracy = 98.51%: 100%|██████████| 13/13 [00:00<00:00, 21.37it/s]\n",
      "[39 / 50] Train: Loss = 0.03134, Accuracy = 98.67%: 100%|██████████| 572/572 [00:12<00:00, 44.32it/s]\n",
      "[39 / 50]   Val: Loss = 0.13358, Accuracy = 98.58%: 100%|██████████| 13/13 [00:00<00:00, 19.53it/s]\n",
      "[40 / 50] Train: Loss = 0.03105, Accuracy = 98.68%: 100%|██████████| 572/572 [00:12<00:00, 44.35it/s]\n",
      "[40 / 50]   Val: Loss = 0.13307, Accuracy = 98.57%: 100%|██████████| 13/13 [00:00<00:00, 20.09it/s]\n",
      "[41 / 50] Train: Loss = 0.03081, Accuracy = 98.70%: 100%|██████████| 572/572 [00:12<00:00, 44.90it/s]\n",
      "[41 / 50]   Val: Loss = 0.14364, Accuracy = 98.53%: 100%|██████████| 13/13 [00:00<00:00, 20.83it/s]\n",
      "[42 / 50] Train: Loss = 0.03091, Accuracy = 98.69%: 100%|██████████| 572/572 [00:12<00:00, 45.07it/s]\n",
      "[42 / 50]   Val: Loss = 0.13813, Accuracy = 98.53%: 100%|██████████| 13/13 [00:00<00:00, 21.11it/s]\n",
      "[43 / 50] Train: Loss = 0.03110, Accuracy = 98.68%: 100%|██████████| 572/572 [00:13<00:00, 43.85it/s]\n",
      "[43 / 50]   Val: Loss = 0.13731, Accuracy = 98.61%: 100%|██████████| 13/13 [00:00<00:00, 19.10it/s]\n",
      "[44 / 50] Train: Loss = 0.03103, Accuracy = 98.68%: 100%|██████████| 572/572 [00:12<00:00, 44.90it/s]\n",
      "[44 / 50]   Val: Loss = 0.14650, Accuracy = 98.50%: 100%|██████████| 13/13 [00:00<00:00, 21.55it/s]\n",
      "[45 / 50] Train: Loss = 0.03117, Accuracy = 98.68%: 100%|██████████| 572/572 [00:12<00:00, 45.01it/s]\n",
      "[45 / 50]   Val: Loss = 0.13801, Accuracy = 98.57%: 100%|██████████| 13/13 [00:00<00:00, 20.13it/s]\n",
      "[46 / 50] Train: Loss = 0.03071, Accuracy = 98.70%: 100%|██████████| 572/572 [00:12<00:00, 44.55it/s]\n",
      "[46 / 50]   Val: Loss = 0.13861, Accuracy = 98.55%: 100%|██████████| 13/13 [00:00<00:00, 20.47it/s]\n",
      "[47 / 50] Train: Loss = 0.03061, Accuracy = 98.69%: 100%|██████████| 572/572 [00:12<00:00, 44.13it/s]\n",
      "[47 / 50]   Val: Loss = 0.12868, Accuracy = 98.57%: 100%|██████████| 13/13 [00:00<00:00, 21.08it/s]\n",
      "[48 / 50] Train: Loss = 0.03086, Accuracy = 98.68%: 100%|██████████| 572/572 [00:12<00:00, 44.38it/s]\n",
      "[48 / 50]   Val: Loss = 0.14062, Accuracy = 98.56%: 100%|██████████| 13/13 [00:00<00:00, 20.61it/s]\n",
      "[49 / 50] Train: Loss = 0.03065, Accuracy = 98.69%: 100%|██████████| 572/572 [00:13<00:00, 43.90it/s]\n",
      "[49 / 50]   Val: Loss = 0.14372, Accuracy = 98.54%: 100%|██████████| 13/13 [00:00<00:00, 22.31it/s]\n",
      "[50 / 50] Train: Loss = 0.03086, Accuracy = 98.69%: 100%|██████████| 572/572 [00:12<00:00, 44.04it/s]\n",
      "[50 / 50]   Val: Loss = 0.14603, Accuracy = 98.53%: 100%|██████████| 13/13 [00:00<00:00, 22.31it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9305158209654042"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count = 0\n",
    "sum_count = 0\n",
    "\n",
    "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
    "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "    logits = model(X_batch)\n",
    "\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    mask = (y_batch != 0).float()\n",
    "    cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
    "    cur_sum_count = mask.sum().item()\n",
    "                \n",
    "    correct_count += cur_correct_count\n",
    "    sum_count += cur_sum_count\n",
    "\n",
    "correct_count / sum_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.55782, Accuracy = 82.35%: 100%|██████████| 572/572 [00:14<00:00, 38.19it/s]\n",
      "[1 / 50]   Val: Loss = 0.27731, Accuracy = 90.93%: 100%|██████████| 13/13 [00:03<00:00,  4.06it/s]\n",
      "[2 / 50] Train: Loss = 0.20531, Accuracy = 93.45%: 100%|██████████| 572/572 [00:14<00:00, 39.03it/s]\n",
      "[2 / 50]   Val: Loss = 0.18185, Accuracy = 94.23%: 100%|██████████| 13/13 [00:02<00:00,  5.35it/s]\n",
      "[3 / 50] Train: Loss = 0.12987, Accuracy = 95.98%: 100%|██████████| 572/572 [00:14<00:00, 38.83it/s]\n",
      "[3 / 50]   Val: Loss = 0.14801, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 15.86it/s]\n",
      "[4 / 50] Train: Loss = 0.08872, Accuracy = 97.30%: 100%|██████████| 572/572 [00:14<00:00, 39.07it/s]\n",
      "[4 / 50]   Val: Loss = 0.12792, Accuracy = 95.88%: 100%|██████████| 13/13 [00:00<00:00, 15.52it/s]\n",
      "[5 / 50] Train: Loss = 0.06160, Accuracy = 98.17%: 100%|██████████| 572/572 [00:15<00:00, 37.06it/s]\n",
      "[5 / 50]   Val: Loss = 0.11877, Accuracy = 96.15%: 100%|██████████| 13/13 [00:00<00:00, 14.28it/s]\n",
      "[6 / 50] Train: Loss = 0.04215, Accuracy = 98.79%: 100%|██████████| 572/572 [00:16<00:00, 34.75it/s]\n",
      "[6 / 50]   Val: Loss = 0.12572, Accuracy = 96.18%: 100%|██████████| 13/13 [00:00<00:00, 14.47it/s]\n",
      "[7 / 50] Train: Loss = 0.02831, Accuracy = 99.23%: 100%|██████████| 572/572 [00:16<00:00, 35.70it/s] \n",
      "[7 / 50]   Val: Loss = 0.12225, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 15.26it/s]\n",
      "[8 / 50] Train: Loss = 0.01876, Accuracy = 99.51%: 100%|██████████| 572/572 [00:16<00:00, 35.46it/s]\n",
      "[8 / 50]   Val: Loss = 0.13059, Accuracy = 96.41%: 100%|██████████| 13/13 [00:00<00:00, 13.33it/s]\n",
      "[9 / 50] Train: Loss = 0.01210, Accuracy = 99.71%: 100%|██████████| 572/572 [00:16<00:00, 34.36it/s] \n",
      "[9 / 50]   Val: Loss = 0.13371, Accuracy = 96.32%: 100%|██████████| 13/13 [00:00<00:00, 14.80it/s]\n",
      "[10 / 50] Train: Loss = 0.00785, Accuracy = 99.84%: 100%|██████████| 572/572 [00:16<00:00, 35.22it/s] \n",
      "[10 / 50]   Val: Loss = 0.14452, Accuracy = 96.29%: 100%|██████████| 13/13 [00:00<00:00, 13.76it/s]\n",
      "[11 / 50] Train: Loss = 0.00484, Accuracy = 99.92%: 100%|██████████| 572/572 [00:16<00:00, 35.16it/s] \n",
      "[11 / 50]   Val: Loss = 0.15259, Accuracy = 96.39%: 100%|██████████| 13/13 [00:00<00:00, 13.71it/s]\n",
      "[12 / 50] Train: Loss = 0.00300, Accuracy = 99.96%: 100%|██████████| 572/572 [00:16<00:00, 34.24it/s] \n",
      "[12 / 50]   Val: Loss = 0.15942, Accuracy = 96.34%: 100%|██████████| 13/13 [00:00<00:00, 14.16it/s]\n",
      "[13 / 50] Train: Loss = 0.00193, Accuracy = 99.98%: 100%|██████████| 572/572 [00:17<00:00, 33.10it/s] \n",
      "[13 / 50]   Val: Loss = 0.16634, Accuracy = 96.37%: 100%|██████████| 13/13 [00:00<00:00, 13.67it/s]\n",
      "[14 / 50] Train: Loss = 0.00155, Accuracy = 99.98%: 100%|██████████| 572/572 [00:17<00:00, 33.44it/s] \n",
      "[14 / 50]   Val: Loss = 0.17710, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 13.70it/s]\n",
      "[15 / 50] Train: Loss = 0.00126, Accuracy = 99.98%: 100%|██████████| 572/572 [00:17<00:00, 33.49it/s] \n",
      "[15 / 50]   Val: Loss = 0.18857, Accuracy = 96.37%: 100%|██████████| 13/13 [00:00<00:00, 14.42it/s]\n",
      "[16 / 50] Train: Loss = 0.00134, Accuracy = 99.98%: 100%|██████████| 572/572 [00:15<00:00, 35.94it/s] \n",
      "[16 / 50]   Val: Loss = 0.19435, Accuracy = 96.32%: 100%|██████████| 13/13 [00:00<00:00, 14.92it/s]\n",
      "[17 / 50] Train: Loss = 0.00382, Accuracy = 99.89%: 100%|██████████| 572/572 [00:17<00:00, 32.39it/s] \n",
      "[17 / 50]   Val: Loss = 0.19253, Accuracy = 96.34%: 100%|██████████| 13/13 [00:00<00:00, 13.86it/s]\n",
      "[18 / 50] Train: Loss = 0.00122, Accuracy = 99.98%: 100%|██████████| 572/572 [00:17<00:00, 32.86it/s] \n",
      "[18 / 50]   Val: Loss = 0.19400, Accuracy = 96.46%: 100%|██████████| 13/13 [00:00<00:00, 13.85it/s]\n",
      "[19 / 50] Train: Loss = 0.00041, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.98it/s]\n",
      "[19 / 50]   Val: Loss = 0.19466, Accuracy = 96.49%: 100%|██████████| 13/13 [00:01<00:00, 12.59it/s]\n",
      "[20 / 50] Train: Loss = 0.00020, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 31.68it/s]\n",
      "[20 / 50]   Val: Loss = 0.19972, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 13.45it/s]\n",
      "[21 / 50] Train: Loss = 0.00015, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 31.92it/s]\n",
      "[21 / 50]   Val: Loss = 0.20174, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 13.32it/s]\n",
      "[22 / 50] Train: Loss = 0.00012, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 31.01it/s]\n",
      "[22 / 50]   Val: Loss = 0.20811, Accuracy = 96.46%: 100%|██████████| 13/13 [00:00<00:00, 13.40it/s]\n",
      "[23 / 50] Train: Loss = 0.00438, Accuracy = 99.85%: 100%|██████████| 572/572 [00:18<00:00, 31.42it/s] \n",
      "[23 / 50]   Val: Loss = 0.20475, Accuracy = 96.34%: 100%|██████████| 13/13 [00:01<00:00, 12.81it/s]\n",
      "[24 / 50] Train: Loss = 0.00195, Accuracy = 99.94%: 100%|██████████| 572/572 [00:17<00:00, 31.94it/s] \n",
      "[24 / 50]   Val: Loss = 0.19968, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 13.46it/s]\n",
      "[25 / 50] Train: Loss = 0.00042, Accuracy = 99.99%: 100%|██████████| 572/572 [00:17<00:00, 32.27it/s] \n",
      "[25 / 50]   Val: Loss = 0.20285, Accuracy = 96.54%: 100%|██████████| 13/13 [00:01<00:00, 12.70it/s]\n",
      "[26 / 50] Train: Loss = 0.00013, Accuracy = 100.00%: 100%|██████████| 572/572 [00:19<00:00, 30.10it/s]\n",
      "[26 / 50]   Val: Loss = 0.20515, Accuracy = 96.56%: 100%|██████████| 13/13 [00:00<00:00, 13.30it/s]\n",
      "[27 / 50] Train: Loss = 0.00009, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 30.65it/s]\n",
      "[27 / 50]   Val: Loss = 0.20657, Accuracy = 96.60%: 100%|██████████| 13/13 [00:01<00:00, 12.81it/s]\n",
      "[28 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:19<00:00, 29.35it/s]\n",
      "[28 / 50]   Val: Loss = 0.21074, Accuracy = 96.59%: 100%|██████████| 13/13 [00:01<00:00, 12.85it/s]\n",
      "[29 / 50] Train: Loss = 0.00008, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 31.39it/s]\n",
      "[29 / 50]   Val: Loss = 0.20964, Accuracy = 96.61%: 100%|██████████| 13/13 [00:00<00:00, 13.03it/s]\n",
      "[30 / 50] Train: Loss = 0.00011, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 30.96it/s]\n",
      "[30 / 50]   Val: Loss = 0.21858, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 13.77it/s]\n",
      "[31 / 50] Train: Loss = 0.00501, Accuracy = 99.83%: 100%|██████████| 572/572 [00:18<00:00, 31.14it/s] \n",
      "[31 / 50]   Val: Loss = 0.22023, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 13.05it/s]\n",
      "[32 / 50] Train: Loss = 0.00101, Accuracy = 99.97%: 100%|██████████| 572/572 [00:18<00:00, 31.11it/s] \n",
      "[32 / 50]   Val: Loss = 0.21859, Accuracy = 96.38%: 100%|██████████| 13/13 [00:00<00:00, 13.11it/s]\n",
      "[33 / 50] Train: Loss = 0.00021, Accuracy = 100.00%: 100%|██████████| 572/572 [00:19<00:00, 29.89it/s]\n",
      "[33 / 50]   Val: Loss = 0.21494, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 13.08it/s]\n",
      "[34 / 50] Train: Loss = 0.00009, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 30.42it/s]\n",
      "[34 / 50]   Val: Loss = 0.21806, Accuracy = 96.50%: 100%|██████████| 13/13 [00:01<00:00, 12.99it/s]\n",
      "[35 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 30.64it/s]\n",
      "[35 / 50]   Val: Loss = 0.21903, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 13.20it/s]\n",
      "[36 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 30.78it/s]\n",
      "[36 / 50]   Val: Loss = 0.22453, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 13.09it/s]\n",
      "[37 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 30.97it/s]\n",
      "[37 / 50]   Val: Loss = 0.22443, Accuracy = 96.53%: 100%|██████████| 13/13 [00:01<00:00, 12.85it/s]\n",
      "[38 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 31.53it/s]\n",
      "[38 / 50]   Val: Loss = 0.22536, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 14.14it/s]\n",
      "[39 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:16<00:00, 33.72it/s]\n",
      "[39 / 50]   Val: Loss = 0.22858, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 13.71it/s]\n",
      "[40 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 32.25it/s]\n",
      "[40 / 50]   Val: Loss = 0.23189, Accuracy = 96.55%: 100%|██████████| 13/13 [00:01<00:00, 12.77it/s]\n",
      "[41 / 50] Train: Loss = 0.00218, Accuracy = 99.93%: 100%|██████████| 572/572 [00:17<00:00, 33.52it/s] \n",
      "[41 / 50]   Val: Loss = 0.26141, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 14.06it/s]\n",
      "[42 / 50] Train: Loss = 0.00359, Accuracy = 99.88%: 100%|██████████| 572/572 [00:16<00:00, 35.60it/s] \n",
      "[42 / 50]   Val: Loss = 0.24515, Accuracy = 96.36%: 100%|██████████| 13/13 [00:00<00:00, 13.61it/s]\n",
      "[43 / 50] Train: Loss = 0.00039, Accuracy = 99.99%: 100%|██████████| 572/572 [00:16<00:00, 33.70it/s] \n",
      "[43 / 50]   Val: Loss = 0.23532, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 13.61it/s]\n",
      "[44 / 50] Train: Loss = 0.00010, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 31.16it/s]\n",
      "[44 / 50]   Val: Loss = 0.23663, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 13.97it/s]\n",
      "[45 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 31.09it/s]\n",
      "[45 / 50]   Val: Loss = 0.23751, Accuracy = 96.53%: 100%|██████████| 13/13 [00:01<00:00, 12.80it/s]\n",
      "[46 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 30.41it/s]\n",
      "[46 / 50]   Val: Loss = 0.24041, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 14.46it/s]\n",
      "[47 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 30.63it/s]\n",
      "[47 / 50]   Val: Loss = 0.24127, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 13.26it/s]\n",
      "[48 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 30.43it/s]\n",
      "[48 / 50]   Val: Loss = 0.24123, Accuracy = 96.55%: 100%|██████████| 13/13 [00:01<00:00, 12.55it/s]\n",
      "[49 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 30.89it/s]\n",
      "[49 / 50]   Val: Loss = 0.24415, Accuracy = 96.57%: 100%|██████████| 13/13 [00:00<00:00, 13.19it/s]\n",
      "[50 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:18<00:00, 30.75it/s]\n",
      "[50 / 50]   Val: Loss = 0.24841, Accuracy = 96.56%: 100%|██████████| 13/13 [00:01<00:00, 12.85it/s]\n",
      "Test: Loss = 0.24621, Accuracy = 96.56%: 100%|██████████| 13/13 [00:00<00:00, 13.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy: 96.56%, Test Loss: 0.24621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9655986071444935"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BidirectionalLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
    "        self.FC = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding = self.embedding(inputs)\n",
    "        out, _ = self.lstm(embedding)\n",
    "        return self.FC(out)\n",
    "\n",
    "model = BidirectionalLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9652252047550686"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count = 0\n",
    "sum_count = 0\n",
    "\n",
    "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
    "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "    logits = model(X_batch)\n",
    "\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    mask = (y_batch != 0).float()\n",
    "    cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
    "    cur_sum_count = mask.sum().item()\n",
    "                \n",
    "    correct_count += cur_correct_count\n",
    "    sum_count += cur_sum_count\n",
    "\n",
    "correct_count / sum_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        embeddings = torch.FloatTensor(embeddings)\n",
    "        self.embedding =nn.Embedding.from_pretrained(embeddings)\n",
    "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, num_layers=lstm_layers_count)\n",
    "        self.FC = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embedding = self.embedding(inputs)\n",
    "        out, _ = self.lstm(embedding)\n",
    "        return self.FC(out)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.74402, Accuracy = 77.96%: 100%|██████████| 572/572 [00:04<00:00, 133.53it/s]\n",
      "[1 / 50]   Val: Loss = 0.36431, Accuracy = 89.36%: 100%|██████████| 13/13 [00:00<00:00, 66.82it/s]\n",
      "[2 / 50] Train: Loss = 0.28088, Accuracy = 91.51%: 100%|██████████| 572/572 [00:03<00:00, 150.40it/s]\n",
      "[2 / 50]   Val: Loss = 0.25432, Accuracy = 92.20%: 100%|██████████| 13/13 [00:00<00:00, 66.62it/s]\n",
      "[3 / 50] Train: Loss = 0.20649, Accuracy = 93.52%: 100%|██████████| 572/572 [00:04<00:00, 131.55it/s]\n",
      "[3 / 50]   Val: Loss = 0.20640, Accuracy = 93.41%: 100%|██████████| 13/13 [00:00<00:00, 53.00it/s]\n",
      "[4 / 50] Train: Loss = 0.16956, Accuracy = 94.55%: 100%|██████████| 572/572 [00:05<00:00, 99.98it/s] \n",
      "[4 / 50]   Val: Loss = 0.18068, Accuracy = 94.17%: 100%|██████████| 13/13 [00:00<00:00, 41.31it/s]\n",
      "[5 / 50] Train: Loss = 0.14913, Accuracy = 95.13%: 100%|██████████| 572/572 [00:06<00:00, 87.19it/s] \n",
      "[5 / 50]   Val: Loss = 0.16627, Accuracy = 94.54%: 100%|██████████| 13/13 [00:00<00:00, 45.02it/s]\n",
      "[6 / 50] Train: Loss = 0.13579, Accuracy = 95.50%: 100%|██████████| 572/572 [00:05<00:00, 96.02it/s] \n",
      "[6 / 50]   Val: Loss = 0.15719, Accuracy = 94.76%: 100%|██████████| 13/13 [00:00<00:00, 40.26it/s]\n",
      "[7 / 50] Train: Loss = 0.12617, Accuracy = 95.79%: 100%|██████████| 572/572 [00:06<00:00, 84.95it/s]\n",
      "[7 / 50]   Val: Loss = 0.15061, Accuracy = 94.97%: 100%|██████████| 13/13 [00:00<00:00, 42.20it/s]\n",
      "[8 / 50] Train: Loss = 0.11937, Accuracy = 95.96%: 100%|██████████| 572/572 [00:07<00:00, 75.15it/s]\n",
      "[8 / 50]   Val: Loss = 0.14581, Accuracy = 95.11%: 100%|██████████| 13/13 [00:00<00:00, 37.49it/s]\n",
      "[9 / 50] Train: Loss = 0.11358, Accuracy = 96.13%: 100%|██████████| 572/572 [00:07<00:00, 80.62it/s]\n",
      "[9 / 50]   Val: Loss = 0.14284, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 36.25it/s]\n",
      "[10 / 50] Train: Loss = 0.10906, Accuracy = 96.24%: 100%|██████████| 572/572 [00:05<00:00, 95.93it/s] \n",
      "[10 / 50]   Val: Loss = 0.13993, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 41.53it/s]\n",
      "[11 / 50] Train: Loss = 0.10561, Accuracy = 96.36%: 100%|██████████| 572/572 [00:05<00:00, 102.04it/s]\n",
      "[11 / 50]   Val: Loss = 0.13833, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 40.39it/s]\n",
      "[12 / 50] Train: Loss = 0.10200, Accuracy = 96.45%: 100%|██████████| 572/572 [00:06<00:00, 84.28it/s]\n",
      "[12 / 50]   Val: Loss = 0.13604, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 36.76it/s]\n",
      "[13 / 50] Train: Loss = 0.09937, Accuracy = 96.53%: 100%|██████████| 572/572 [00:06<00:00, 83.55it/s] \n",
      "[13 / 50]   Val: Loss = 0.13578, Accuracy = 95.47%: 100%|██████████| 13/13 [00:00<00:00, 38.04it/s]\n",
      "[14 / 50] Train: Loss = 0.09684, Accuracy = 96.63%: 100%|██████████| 572/572 [00:07<00:00, 77.33it/s]\n",
      "[14 / 50]   Val: Loss = 0.13379, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 41.52it/s]\n",
      "[15 / 50] Train: Loss = 0.09444, Accuracy = 96.67%: 100%|██████████| 572/572 [00:08<00:00, 68.53it/s]\n",
      "[15 / 50]   Val: Loss = 0.13350, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 36.56it/s]\n",
      "[16 / 50] Train: Loss = 0.09241, Accuracy = 96.76%: 100%|██████████| 572/572 [00:08<00:00, 68.91it/s]\n",
      "[16 / 50]   Val: Loss = 0.13334, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 34.39it/s]\n",
      "[17 / 50] Train: Loss = 0.09046, Accuracy = 96.81%: 100%|██████████| 572/572 [00:07<00:00, 80.09it/s] \n",
      "[17 / 50]   Val: Loss = 0.13389, Accuracy = 95.47%: 100%|██████████| 13/13 [00:00<00:00, 28.62it/s]\n",
      "[18 / 50] Train: Loss = 0.08882, Accuracy = 96.87%: 100%|██████████| 572/572 [00:07<00:00, 72.79it/s]\n",
      "[18 / 50]   Val: Loss = 0.13220, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 36.12it/s]\n",
      "[19 / 50] Train: Loss = 0.08752, Accuracy = 96.91%: 100%|██████████| 572/572 [00:07<00:00, 77.71it/s]\n",
      "[19 / 50]   Val: Loss = 0.13322, Accuracy = 95.56%: 100%|██████████| 13/13 [00:00<00:00, 37.80it/s]\n",
      "[20 / 50] Train: Loss = 0.08585, Accuracy = 96.98%: 100%|██████████| 572/572 [00:06<00:00, 88.21it/s]\n",
      "[20 / 50]   Val: Loss = 0.13269, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 35.59it/s]\n",
      "[21 / 50] Train: Loss = 0.08453, Accuracy = 97.01%: 100%|██████████| 572/572 [00:07<00:00, 73.81it/s]\n",
      "[21 / 50]   Val: Loss = 0.13160, Accuracy = 95.48%: 100%|██████████| 13/13 [00:00<00:00, 35.37it/s]\n",
      "[22 / 50] Train: Loss = 0.08316, Accuracy = 97.05%: 100%|██████████| 572/572 [00:10<00:00, 54.55it/s]\n",
      "[22 / 50]   Val: Loss = 0.13174, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 20.97it/s]\n",
      "[23 / 50] Train: Loss = 0.08189, Accuracy = 97.09%: 100%|██████████| 572/572 [00:07<00:00, 81.62it/s]\n",
      "[23 / 50]   Val: Loss = 0.13230, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 38.77it/s]\n",
      "[24 / 50] Train: Loss = 0.08081, Accuracy = 97.13%: 100%|██████████| 572/572 [00:06<00:00, 87.52it/s]\n",
      "[24 / 50]   Val: Loss = 0.13276, Accuracy = 95.48%: 100%|██████████| 13/13 [00:00<00:00, 37.85it/s]\n",
      "[25 / 50] Train: Loss = 0.07974, Accuracy = 97.17%: 100%|██████████| 572/572 [00:06<00:00, 86.37it/s]\n",
      "[25 / 50]   Val: Loss = 0.13261, Accuracy = 95.56%: 100%|██████████| 13/13 [00:00<00:00, 38.90it/s]\n",
      "[26 / 50] Train: Loss = 0.07868, Accuracy = 97.20%: 100%|██████████| 572/572 [00:07<00:00, 80.35it/s]\n",
      "[26 / 50]   Val: Loss = 0.13377, Accuracy = 95.52%: 100%|██████████| 13/13 [00:00<00:00, 31.34it/s]\n",
      "[27 / 50] Train: Loss = 0.07773, Accuracy = 97.23%: 100%|██████████| 572/572 [00:07<00:00, 73.93it/s]\n",
      "[27 / 50]   Val: Loss = 0.13396, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 39.44it/s]\n",
      "[28 / 50] Train: Loss = 0.07676, Accuracy = 97.26%: 100%|██████████| 572/572 [00:06<00:00, 83.57it/s]\n",
      "[28 / 50]   Val: Loss = 0.13367, Accuracy = 95.61%: 100%|██████████| 13/13 [00:00<00:00, 33.95it/s]\n",
      "[29 / 50] Train: Loss = 0.07600, Accuracy = 97.29%: 100%|██████████| 572/572 [00:07<00:00, 81.62it/s]\n",
      "[29 / 50]   Val: Loss = 0.13253, Accuracy = 95.60%: 100%|██████████| 13/13 [00:00<00:00, 35.77it/s]\n",
      "[30 / 50] Train: Loss = 0.07505, Accuracy = 97.30%: 100%|██████████| 572/572 [00:07<00:00, 78.53it/s]\n",
      "[30 / 50]   Val: Loss = 0.13545, Accuracy = 95.44%: 100%|██████████| 13/13 [00:00<00:00, 38.44it/s]\n",
      "[31 / 50] Train: Loss = 0.07408, Accuracy = 97.37%: 100%|██████████| 572/572 [00:06<00:00, 90.33it/s]\n",
      "[31 / 50]   Val: Loss = 0.13623, Accuracy = 95.48%: 100%|██████████| 13/13 [00:00<00:00, 43.89it/s]\n",
      "[32 / 50] Train: Loss = 0.07339, Accuracy = 97.39%: 100%|██████████| 572/572 [00:05<00:00, 102.78it/s]\n",
      "[32 / 50]   Val: Loss = 0.13675, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 46.12it/s]\n",
      "[33 / 50] Train: Loss = 0.07249, Accuracy = 97.41%: 100%|██████████| 572/572 [00:05<00:00, 101.93it/s]\n",
      "[33 / 50]   Val: Loss = 0.13800, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 43.92it/s]\n",
      "[34 / 50] Train: Loss = 0.07196, Accuracy = 97.43%: 100%|██████████| 572/572 [00:05<00:00, 102.49it/s]\n",
      "[34 / 50]   Val: Loss = 0.13656, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 41.87it/s]\n",
      "[35 / 50] Train: Loss = 0.07095, Accuracy = 97.46%: 100%|██████████| 572/572 [00:05<00:00, 97.81it/s] \n",
      "[35 / 50]   Val: Loss = 0.13881, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 37.71it/s]\n",
      "[36 / 50] Train: Loss = 0.07035, Accuracy = 97.48%: 100%|██████████| 572/572 [00:07<00:00, 79.40it/s] \n",
      "[36 / 50]   Val: Loss = 0.13646, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 33.10it/s]\n",
      "[37 / 50] Train: Loss = 0.06929, Accuracy = 97.52%: 100%|██████████| 572/572 [00:07<00:00, 75.89it/s]\n",
      "[37 / 50]   Val: Loss = 0.13714, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 37.10it/s]\n",
      "[38 / 50] Train: Loss = 0.06890, Accuracy = 97.54%: 100%|██████████| 572/572 [00:06<00:00, 83.89it/s]\n",
      "[38 / 50]   Val: Loss = 0.13936, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 37.88it/s]\n",
      "[39 / 50] Train: Loss = 0.06836, Accuracy = 97.55%: 100%|██████████| 572/572 [00:07<00:00, 77.31it/s]\n",
      "[39 / 50]   Val: Loss = 0.13939, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 35.33it/s]\n",
      "[40 / 50] Train: Loss = 0.06752, Accuracy = 97.59%: 100%|██████████| 572/572 [00:07<00:00, 78.36it/s]\n",
      "[40 / 50]   Val: Loss = 0.14094, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 36.68it/s]\n",
      "[41 / 50] Train: Loss = 0.06682, Accuracy = 97.60%: 100%|██████████| 572/572 [00:07<00:00, 76.66it/s]\n",
      "[41 / 50]   Val: Loss = 0.14040, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 34.73it/s]\n",
      "[42 / 50] Train: Loss = 0.06624, Accuracy = 97.62%: 100%|██████████| 572/572 [00:07<00:00, 74.84it/s]\n",
      "[42 / 50]   Val: Loss = 0.14105, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 45.60it/s]\n",
      "[43 / 50] Train: Loss = 0.06569, Accuracy = 97.65%: 100%|██████████| 572/572 [00:07<00:00, 75.13it/s]\n",
      "[43 / 50]   Val: Loss = 0.14144, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 32.63it/s]\n",
      "[44 / 50] Train: Loss = 0.06508, Accuracy = 97.67%: 100%|██████████| 572/572 [00:07<00:00, 72.10it/s]\n",
      "[44 / 50]   Val: Loss = 0.14193, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 30.32it/s]\n",
      "[45 / 50] Train: Loss = 0.06455, Accuracy = 97.68%: 100%|██████████| 572/572 [00:08<00:00, 70.80it/s]\n",
      "[45 / 50]   Val: Loss = 0.14319, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 45.29it/s]\n",
      "[46 / 50] Train: Loss = 0.06404, Accuracy = 97.70%: 100%|██████████| 572/572 [00:07<00:00, 78.26it/s]\n",
      "[46 / 50]   Val: Loss = 0.14577, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 40.84it/s]\n",
      "[47 / 50] Train: Loss = 0.06354, Accuracy = 97.72%: 100%|██████████| 572/572 [00:08<00:00, 70.07it/s]\n",
      "[47 / 50]   Val: Loss = 0.14687, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 48.32it/s]\n",
      "[48 / 50] Train: Loss = 0.06306, Accuracy = 97.74%: 100%|██████████| 572/572 [00:06<00:00, 85.49it/s]\n",
      "[48 / 50]   Val: Loss = 0.14609, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 27.27it/s]\n",
      "[49 / 50] Train: Loss = 0.06240, Accuracy = 97.76%: 100%|██████████| 572/572 [00:11<00:00, 51.07it/s]\n",
      "[49 / 50]   Val: Loss = 0.14555, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 42.10it/s]\n",
      "[50 / 50] Train: Loss = 0.06213, Accuracy = 97.78%: 100%|██████████| 572/572 [00:07<00:00, 72.15it/s]\n",
      "[50 / 50]   Val: Loss = 0.14978, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 37.40it/s]\n",
      "Test: Loss = 0.15020, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 43.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy: 95.23%, Test Loss: 0.15020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9522962261557494"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9517883147599228"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count = 0\n",
    "sum_count = 0\n",
    "\n",
    "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
    "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "    logits = model(X_batch)\n",
    "\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    mask = (y_batch != 0).float()\n",
    "    cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
    "    cur_sum_count = mask.sum().item()\n",
    "                \n",
    "    correct_count += cur_correct_count\n",
    "    sum_count += cur_sum_count\n",
    "\n",
    "correct_count / sum_count"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
